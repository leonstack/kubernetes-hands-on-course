# Kubernetes è°ƒåº¦å™¨ä»‹ç»

> **ç‰ˆæœ¬è¯´æ˜**ï¼šæœ¬æ–‡æ¡£åŸºäº Kubernetes v1.27+ ç‰ˆæœ¬ç¼–å†™ï¼Œä½¿ç”¨ç¨³å®šçš„ `kubescheduler.config.k8s.io/v1` APIã€‚æ‰€æœ‰é…ç½®ç¤ºä¾‹å’Œä»£ç å‡å·²é’ˆå¯¹è¯¥ç‰ˆæœ¬è¿›è¡Œä¼˜åŒ–ã€‚

## ç›®å½•

- [Kubernetes è°ƒåº¦å™¨ä»‹ç»](#kubernetes-è°ƒåº¦å™¨ä»‹ç»)
  - [ç›®å½•](#ç›®å½•)
  - [0. å¿«é€Ÿå¼€å§‹](#0-å¿«é€Ÿå¼€å§‹)
    - [0.1 5åˆ†é’Ÿäº†è§£è°ƒåº¦å™¨](#01-5åˆ†é’Ÿäº†è§£è°ƒåº¦å™¨)
    - [0.2 å¸¸ç”¨è°ƒåº¦é…ç½®ç¤ºä¾‹](#02-å¸¸ç”¨è°ƒåº¦é…ç½®ç¤ºä¾‹)
      - [0.2.1 åŸºç¡€èµ„æºè°ƒåº¦](#021-åŸºç¡€èµ„æºè°ƒåº¦)
      - [0.2.2 èŠ‚ç‚¹é€‰æ‹©é…ç½®](#022-èŠ‚ç‚¹é€‰æ‹©é…ç½®)
      - [0.2.3 é«˜å¯ç”¨éƒ¨ç½²é…ç½®](#023-é«˜å¯ç”¨éƒ¨ç½²é…ç½®)
      - [0.2.4 ç”Ÿäº§çº§è°ƒåº¦å™¨é…ç½®](#024-ç”Ÿäº§çº§è°ƒåº¦å™¨é…ç½®)
    - [0.3 è°ƒåº¦é—®é¢˜å¿«é€Ÿè¯Šæ–­](#03-è°ƒåº¦é—®é¢˜å¿«é€Ÿè¯Šæ–­)
      - [0.3.1 åŸºç¡€è¯Šæ–­å‘½ä»¤](#031-åŸºç¡€è¯Šæ–­å‘½ä»¤)
      - [0.3.2 å¸¸è§é—®é¢˜å¿«é€Ÿæ£€æŸ¥è¡¨](#032-å¸¸è§é—®é¢˜å¿«é€Ÿæ£€æŸ¥è¡¨)
      - [0.3.3 è°ƒåº¦å¤±è´¥äº‹ä»¶è§£è¯»](#033-è°ƒåº¦å¤±è´¥äº‹ä»¶è§£è¯»)
  - [1. Kubernetes è°ƒåº¦å™¨æ¦‚è¿°](#1-kubernetes-è°ƒåº¦å™¨æ¦‚è¿°)
    - [1.1 è°ƒåº¦å™¨åœ¨ Kubernetes æ¶æ„ä¸­çš„ä½ç½®å’Œä½œç”¨](#11-è°ƒåº¦å™¨åœ¨-kubernetes-æ¶æ„ä¸­çš„ä½ç½®å’Œä½œç”¨)
    - [1.2 è°ƒåº¦å™¨çš„åŸºæœ¬å·¥ä½œæµç¨‹](#12-è°ƒåº¦å™¨çš„åŸºæœ¬å·¥ä½œæµç¨‹)
    - [1.3 é»˜è®¤è°ƒåº¦å™¨çš„è®¾è®¡åŸç†](#13-é»˜è®¤è°ƒåº¦å™¨çš„è®¾è®¡åŸç†)
    - [1.4 è°ƒåº¦ç†è®ºåŸºç¡€](#14-è°ƒåº¦ç†è®ºåŸºç¡€)
      - [1.4.1 è°ƒåº¦é—®é¢˜çš„æ•°å­¦å»ºæ¨¡](#141-è°ƒåº¦é—®é¢˜çš„æ•°å­¦å»ºæ¨¡)
      - [1.4.2 å¤šç›®æ ‡ä¼˜åŒ–ç†è®º](#142-å¤šç›®æ ‡ä¼˜åŒ–ç†è®º)
      - [1.4.3 è°ƒåº¦ç®—æ³•çš„è®¡ç®—å¤æ‚åº¦](#143-è°ƒåº¦ç®—æ³•çš„è®¡ç®—å¤æ‚åº¦)
      - [1.4.4 è°ƒåº¦å™¨æ€§èƒ½ä¼˜åŒ–](#144-è°ƒåº¦å™¨æ€§èƒ½ä¼˜åŒ–)
  - [2. è°ƒåº¦è¿‡ç¨‹è¯¦è§£](#2-è°ƒåº¦è¿‡ç¨‹è¯¦è§£)
    - [2.1 è°ƒåº¦æµæ°´çº¿ï¼šè¿‡æ»¤(Filtering)å’Œè¯„åˆ†(Scoring)](#21-è°ƒåº¦æµæ°´çº¿è¿‡æ»¤filteringå’Œè¯„åˆ†scoring)
      - [2.1.1 è¿‡æ»¤é˜¶æ®µï¼ˆFiltering Phaseï¼‰](#211-è¿‡æ»¤é˜¶æ®µfiltering-phase)
      - [2.1.2 è¯„åˆ†é˜¶æ®µï¼ˆScoring Phaseï¼‰](#212-è¯„åˆ†é˜¶æ®µscoring-phase)
    - [2.2 å¸¸è§çš„è¿‡æ»¤ç­–ç•¥](#22-å¸¸è§çš„è¿‡æ»¤ç­–ç•¥)
      - [2.2.1 NodeResourcesFit](#221-noderesourcesfit)
      - [2.2.2 NodeAffinity](#222-nodeaffinity)
      - [2.2.3 PodTopologySpread](#223-podtopologyspread)
      - [2.2.4 TaintToleration](#224-tainttoleration)
    - [2.3 å¸¸è§çš„è¯„åˆ†ç­–ç•¥](#23-å¸¸è§çš„è¯„åˆ†ç­–ç•¥)
      - [2.3.1 NodeResourcesFit](#231-noderesourcesfit)
      - [2.3.2 NodeAffinity](#232-nodeaffinity)
      - [2.3.3 InterPodAffinity](#233-interpodaffinity)
    - [2.4 èŠ‚ç‚¹äº²å’Œæ€§ä¸åäº²å’Œæ€§](#24-èŠ‚ç‚¹äº²å’Œæ€§ä¸åäº²å’Œæ€§)
      - [2.4.1 èŠ‚ç‚¹äº²å’Œæ€§ï¼ˆNode Affinityï¼‰](#241-èŠ‚ç‚¹äº²å’Œæ€§node-affinity)
    - [2.5 Pod äº²å’Œæ€§ä¸åäº²å’Œæ€§](#25-pod-äº²å’Œæ€§ä¸åäº²å’Œæ€§)
      - [2.5.1 Pod äº²å’Œæ€§ï¼ˆPod Affinityï¼‰](#251-pod-äº²å’Œæ€§pod-affinity)
      - [2.5.2 Pod åäº²å’Œæ€§ï¼ˆPod Anti-Affinityï¼‰](#252-pod-åäº²å’Œæ€§pod-anti-affinity)
    - [2.6 æ±¡ç‚¹(Taints)å’Œå®¹å¿(Tolerations)](#26-æ±¡ç‚¹taintså’Œå®¹å¿tolerations)
      - [2.6.1 æ±¡ç‚¹ï¼ˆTaintsï¼‰](#261-æ±¡ç‚¹taints)
      - [2.6.2 å®¹å¿ï¼ˆTolerationsï¼‰](#262-å®¹å¿tolerations)
  - [3. è°ƒåº¦å™¨é…ç½®ä¸è‡ªå®šä¹‰](#3-è°ƒåº¦å™¨é…ç½®ä¸è‡ªå®šä¹‰)
    - [3.1 è°ƒåº¦é…ç½®æ–‡ä»¶ä»‹ç»](#31-è°ƒåº¦é…ç½®æ–‡ä»¶ä»‹ç»)
      - [3.1.1 åŸºæœ¬é…ç½®ç»“æ„](#311-åŸºæœ¬é…ç½®ç»“æ„)
      - [3.1.2 å¸¸ç”¨é…ç½®é€‰é¡¹](#312-å¸¸ç”¨é…ç½®é€‰é¡¹)
    - [3.2 è°ƒåº¦æ’ä»¶æ¡†æ¶](#32-è°ƒåº¦æ’ä»¶æ¡†æ¶)
      - [3.2.1 æ’ä»¶æ‰©å±•ç‚¹è¯¦è§£](#321-æ’ä»¶æ‰©å±•ç‚¹è¯¦è§£)
      - [3.2.2 è‡ªå®šä¹‰æ’ä»¶å¼€å‘](#322-è‡ªå®šä¹‰æ’ä»¶å¼€å‘)
    - [3.3 å¤šè°ƒåº¦å™¨éƒ¨ç½²](#33-å¤šè°ƒåº¦å™¨éƒ¨ç½²)
      - [3.3.1 éƒ¨ç½²é¢å¤–è°ƒåº¦å™¨](#331-éƒ¨ç½²é¢å¤–è°ƒåº¦å™¨)
      - [3.3.2 æŒ‡å®šè°ƒåº¦å™¨](#332-æŒ‡å®šè°ƒåº¦å™¨)
      - [3.3.3 è°ƒåº¦å™¨é€‰æ‹©ç­–ç•¥](#333-è°ƒåº¦å™¨é€‰æ‹©ç­–ç•¥)
  - [4. è°ƒåº¦å†³ç­–å®¡è®¡ä¸ç›‘æ§](#4-è°ƒåº¦å†³ç­–å®¡è®¡ä¸ç›‘æ§)
    - [4.1 è°ƒåº¦å†³ç­–å®¡è®¡æœºåˆ¶](#41-è°ƒåº¦å†³ç­–å®¡è®¡æœºåˆ¶)
      - [4.1.1 å®¡è®¡æ—¥å¿—é…ç½®](#411-å®¡è®¡æ—¥å¿—é…ç½®)
      - [4.1.2 è°ƒåº¦äº‹ä»¶è¿½è¸ª](#412-è°ƒåº¦äº‹ä»¶è¿½è¸ª)
      - [4.1.3 è°ƒåº¦æŒ‡æ ‡ç›‘æ§](#413-è°ƒåº¦æŒ‡æ ‡ç›‘æ§)
    - [4.2 é«˜çº§æ•…éšœæ’æŸ¥](#42-é«˜çº§æ•…éšœæ’æŸ¥)
      - [4.2.1 è°ƒåº¦å™¨æ€§èƒ½åˆ†æ](#421-è°ƒåº¦å™¨æ€§èƒ½åˆ†æ)
      - [4.2.2 è°ƒåº¦å¤±è´¥æ·±åº¦åˆ†æ](#422-è°ƒåº¦å¤±è´¥æ·±åº¦åˆ†æ)
      - [4.2.3 è°ƒåº¦å™¨æ—¥å¿—åˆ†æ](#423-è°ƒåº¦å™¨æ—¥å¿—åˆ†æ)
  - [5. æ€»ç»“](#5-æ€»ç»“)
    - [5.1 è°ƒåº¦å™¨æ ¸å¿ƒæ¦‚å¿µå›é¡¾](#51-è°ƒåº¦å™¨æ ¸å¿ƒæ¦‚å¿µå›é¡¾)
    - [5.2 è°ƒåº¦ç†è®ºè¦ç‚¹](#52-è°ƒåº¦ç†è®ºè¦ç‚¹)
    - [5.3 å®è·µæŒ‡å¯¼åŸåˆ™](#53-å®è·µæŒ‡å¯¼åŸåˆ™)
  - [6. æœ¯è¯­è¡¨](#6-æœ¯è¯­è¡¨)
    - [A](#a)
    - [B](#b)
    - [C](#c)
    - [E](#e)
    - [F](#f)
    - [L](#l)
    - [N](#n)
    - [P](#p)
    - [Q](#q)
    - [R](#r)
    - [S](#s)
    - [T](#t)
    - [W](#w)

---

## 0. å¿«é€Ÿå¼€å§‹

### 0.1 5åˆ†é’Ÿäº†è§£è°ƒåº¦å™¨

> **å¿«é€Ÿæ¦‚è§ˆ**ï¼šå¦‚æœä½ æ˜¯ç¬¬ä¸€æ¬¡æ¥è§¦ Kubernetes è°ƒåº¦å™¨ï¼Œè¿™ä¸ªç« èŠ‚å°†å¸®ä½ å¿«é€Ÿå»ºç«‹åŸºæœ¬æ¦‚å¿µã€‚

**è°ƒåº¦å™¨æ˜¯ä»€ä¹ˆï¼Ÿ**

Kubernetes è°ƒåº¦å™¨å°±åƒä¸€ä¸ªæ™ºèƒ½çš„"æˆ¿å±‹ä¸­ä»‹"ï¼Œè´Ÿè´£ä¸ºæ¯ä¸ªæ–°çš„ Podï¼ˆç§Ÿå®¢ï¼‰æ‰¾åˆ°æœ€åˆé€‚çš„ Nodeï¼ˆæˆ¿å±‹ï¼‰ã€‚

```mermaid
flowchart LR
    A["æ–° Pod åˆ›å»º"] --> B["è°ƒåº¦å™¨åˆ†æ"]
    B --> C["é€‰æ‹©æœ€ä½³èŠ‚ç‚¹"]
    C --> D["Pod è¿è¡Œ"]
    
    B1["æ£€æŸ¥èµ„æºéœ€æ±‚"]
    B2["è¯„ä¼°èŠ‚ç‚¹çŠ¶æ€"]
    B3["åº”ç”¨è°ƒåº¦ç­–ç•¥"]
    
    B --> B1
    B --> B2
    B --> B3
```

**æ ¸å¿ƒå·¥ä½œåŸç†ï¼š**

1. **ç›‘å¬**ï¼šè°ƒåº¦å™¨æŒç»­ç›‘å¬ API Server ä¸­æœªåˆ†é…èŠ‚ç‚¹çš„ Pod
2. **è¿‡æ»¤**ï¼šç­›é€‰å‡ºæ»¡è¶³ Pod åŸºæœ¬è¦æ±‚çš„èŠ‚ç‚¹ï¼ˆå¦‚èµ„æºå……è¶³ã€æ»¡è¶³çº¦æŸæ¡ä»¶ï¼‰
3. **è¯„åˆ†**ï¼šå¯¹å€™é€‰èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†ï¼Œé€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
4. **ç»‘å®š**ï¼šå°† Pod åˆ†é…åˆ°é€‰å®šçš„èŠ‚ç‚¹ä¸Š

**å…³é”®æ¦‚å¿µé€Ÿè§ˆï¼š**

| æ¦‚å¿µ | è¯´æ˜ | ç±»æ¯” |
|------|------|------|
| **è¿‡æ»¤ï¼ˆFilteringï¼‰** | æ’é™¤ä¸åˆé€‚çš„èŠ‚ç‚¹ | ç­›é€‰ç¬¦åˆæ¡ä»¶çš„æˆ¿å±‹ |
| **è¯„åˆ†ï¼ˆScoringï¼‰** | ä¸ºå€™é€‰èŠ‚ç‚¹æ‰“åˆ†æ’åº | æ ¹æ®åå¥½ç»™æˆ¿å±‹è¯„åˆ† |
| **äº²å’Œæ€§ï¼ˆAffinityï¼‰** | Pod å¯¹èŠ‚ç‚¹çš„åå¥½ | ç§Ÿå®¢å¯¹æˆ¿å±‹ä½ç½®çš„åå¥½ |
| **æ±¡ç‚¹ï¼ˆTaintï¼‰** | èŠ‚ç‚¹æ‹’ç»æŸäº› Pod | æˆ¿å±‹é™åˆ¶æŸäº›ç§Ÿå®¢ç±»å‹ |
| **å®¹å¿ï¼ˆTolerationï¼‰** | Pod å®¹å¿èŠ‚ç‚¹é™åˆ¶ | ç§Ÿå®¢æ¥å—æˆ¿å±‹çš„æŸäº›é™åˆ¶ |

### 0.2 å¸¸ç”¨è°ƒåº¦é…ç½®ç¤ºä¾‹

> **å®ç”¨é…ç½®**ï¼šä»¥ä¸‹æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­æœ€å¸¸ç”¨çš„è°ƒåº¦é…ç½®æ¨¡å¼ã€‚

#### 0.2.1 åŸºç¡€èµ„æºè°ƒåº¦

```yaml
# åŸºç¡€ Pod èµ„æºè¯·æ±‚
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  containers:
  - name: nginx
    image: nginx:1.20
    resources:
      requests:        # æœ€å°èµ„æºéœ€æ±‚
        cpu: "100m"    # 0.1 CPU æ ¸å¿ƒ
        memory: "128Mi" # 128MB å†…å­˜
      limits:          # æœ€å¤§èµ„æºé™åˆ¶
        cpu: "500m"    # 0.5 CPU æ ¸å¿ƒ
        memory: "512Mi" # 512MB å†…å­˜
```

#### 0.2.2 èŠ‚ç‚¹é€‰æ‹©é…ç½®

```yaml
# æŒ‡å®šèŠ‚ç‚¹ç±»å‹
apiVersion: v1
kind: Pod
metadata:
  name: gpu-workload
spec:
  nodeSelector:
    node-type: gpu    # åªè°ƒåº¦åˆ°æœ‰ GPU çš„èŠ‚ç‚¹
  containers:
  - name: ml-training
    image: tensorflow/tensorflow:latest-gpu
```

#### 0.2.3 é«˜å¯ç”¨éƒ¨ç½²é…ç½®

```yaml
# Pod åäº²å’Œæ€§ç¡®ä¿åˆ†æ•£éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-server
  template:
    metadata:
      labels:
        app: web-server
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-server
            topologyKey: kubernetes.io/hostname  # ç¡®ä¿ä¸åœ¨åŒä¸€èŠ‚ç‚¹
      containers:
      - name: nginx
        image: nginx:1.20
```

#### 0.2.4 ç”Ÿäº§çº§è°ƒåº¦å™¨é…ç½®

```yaml
# å®Œæ•´çš„è°ƒåº¦å™¨é…ç½®ç¤ºä¾‹
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: production-scheduler
  plugins:
    filter:
      enabled:
      - name: NodeResourcesFit
      - name: NodeAffinity
      - name: PodTopologySpread
      - name: TaintToleration
    score:
      enabled:
      - name: NodeResourcesFit
        weight: 1
      - name: NodeAffinity
        weight: 2
      - name: InterPodAffinity
        weight: 1
  pluginConfig:
  - name: NodeResourcesFit
    args:
      scoringStrategy:
        type: LeastAllocated  # ä¼˜å…ˆé€‰æ‹©èµ„æºä½¿ç”¨ç‡ä½çš„èŠ‚ç‚¹
        resources:
        - name: cpu
          weight: 1
        - name: memory
          weight: 1
  - name: PodTopologySpread
    args:
      defaultConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
```

### 0.3 è°ƒåº¦é—®é¢˜å¿«é€Ÿè¯Šæ–­

> ğŸ” **æ•…éšœæ’æŸ¥**ï¼šå½“ Pod æ— æ³•æ­£å¸¸è°ƒåº¦æ—¶ï¼Œä½¿ç”¨è¿™äº›å‘½ä»¤å¿«é€Ÿå®šä½é—®é¢˜ã€‚

#### 0.3.1 åŸºç¡€è¯Šæ–­å‘½ä»¤

```bash
# 1. æŸ¥çœ‹ Pod çŠ¶æ€å’Œäº‹ä»¶
kubectl describe pod <pod-name>

# 2. æŸ¥çœ‹èŠ‚ç‚¹èµ„æºä½¿ç”¨æƒ…å†µ
kubectl top nodes

# 3. æŸ¥çœ‹èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
kubectl describe node <node-name>

# 4. æŸ¥çœ‹è°ƒåº¦å™¨æ—¥å¿—
kubectl logs -n kube-system -l component=kube-scheduler
```

#### 0.3.2 å¸¸è§é—®é¢˜å¿«é€Ÿæ£€æŸ¥è¡¨

| é—®é¢˜ç°è±¡ | å¯èƒ½åŸå›  | æ£€æŸ¥å‘½ä»¤ | è§£å†³æ–¹æ¡ˆ |
|----------|----------|----------|----------|
| Pod ä¸€ç›´ Pending | èµ„æºä¸è¶³ | `kubectl top nodes` | å¢åŠ èŠ‚ç‚¹æˆ–å‡å°‘èµ„æºè¯·æ±‚ |
| Pod ä¸€ç›´ Pending | èŠ‚ç‚¹é€‰æ‹©å™¨ä¸åŒ¹é… | `kubectl get nodes --show-labels` | æ£€æŸ¥èŠ‚ç‚¹æ ‡ç­¾å’Œé€‰æ‹©å™¨ |
| Pod ä¸€ç›´ Pending | æ±¡ç‚¹æ— æ³•å®¹å¿ | `kubectl describe node <node>` | æ·»åŠ å®¹å¿æˆ–ç§»é™¤æ±¡ç‚¹ |
| è°ƒåº¦ç¼“æ…¢ | è°ƒåº¦å™¨æ€§èƒ½é—®é¢˜ | `kubectl logs kube-scheduler` | æ£€æŸ¥è°ƒåº¦å™¨é…ç½®å’Œèµ„æº |

#### 0.3.3 è°ƒåº¦å¤±è´¥äº‹ä»¶è§£è¯»

```bash
# æŸ¥çœ‹ Pod äº‹ä»¶
kubectl describe pod my-pod

# å¸¸è§é”™è¯¯ä¿¡æ¯åŠå«ä¹‰ï¼š
```

**å¸¸è§é”™è¯¯ä¿¡æ¯ï¼š**

- `Insufficient cpu`ï¼šèŠ‚ç‚¹ CPU èµ„æºä¸è¶³
- `Insufficient memory`ï¼šèŠ‚ç‚¹å†…å­˜èµ„æºä¸è¶³
- `node(s) didn't match node selector`ï¼šæ²¡æœ‰èŠ‚ç‚¹åŒ¹é…é€‰æ‹©å™¨
- `node(s) had taint that the pod didn't tolerate`ï¼šèŠ‚ç‚¹æœ‰æ±¡ç‚¹ä½† Pod æ— æ³•å®¹å¿
- `pod has unbound immediate PersistentVolumeClaims`ï¼šå­˜å‚¨å·æ— æ³•ç»‘å®š

> **æ³¨æ„**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¿®æ”¹è°ƒåº¦å™¨é…ç½®å‰ï¼Œå»ºè®®å…ˆåœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯ã€‚
> **æç¤º**ï¼šä½¿ç”¨ `kubectl get events --sort-by=.metadata.creationTimestamp` å¯ä»¥æŒ‰æ—¶é—´é¡ºåºæŸ¥çœ‹é›†ç¾¤äº‹ä»¶ã€‚

---

## 1. Kubernetes è°ƒåº¦å™¨æ¦‚è¿°

### 1.1 è°ƒåº¦å™¨åœ¨ Kubernetes æ¶æ„ä¸­çš„ä½ç½®å’Œä½œç”¨

Kubernetes è°ƒåº¦å™¨ï¼ˆkube-schedulerï¼‰æ˜¯ Kubernetes æ§åˆ¶å¹³é¢çš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ï¼Œè´Ÿè´£ä¸ºæ–°åˆ›å»ºçš„ Pod é€‰æ‹©åˆé€‚çš„èŠ‚ç‚¹è¿›è¡Œéƒ¨ç½²ã€‚è°ƒåº¦å™¨åœ¨æ•´ä¸ª Kubernetes æ¶æ„ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼š

- **ä½ç½®**ï¼šè°ƒåº¦å™¨ä½œä¸ºç‹¬ç«‹çš„æ§åˆ¶å¹³é¢ç»„ä»¶è¿è¡Œï¼Œé€šå¸¸éƒ¨ç½²åœ¨ Master èŠ‚ç‚¹ä¸Š
- **ä½œç”¨**ï¼šç›‘å¬ API Server ä¸­æœªè°ƒåº¦çš„ Podï¼Œæ ¹æ®è°ƒåº¦ç­–ç•¥ä¸ºå…¶é€‰æ‹©æœ€ä¼˜çš„èŠ‚ç‚¹
- **äº¤äº’**ï¼šä¸ API Serverã€kubelet ç­‰ç»„ä»¶åä½œå®Œæˆ Pod çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†

```mermaid
graph TB
    subgraph "Control Plane"
        API["API Server"]
        SCHED["kube-scheduler"]
        CM["Controller Manager"]
        ETCD["etcd"]
    end
    
    subgraph "Worker Nodes"
        N1["Node 1<br/>kubelet"]
        N2["Node 2<br/>kubelet"]
        N3["Node 3<br/>kubelet"]
    end
    
    USER["ç”¨æˆ·/åº”ç”¨"] --> API
    API --> SCHED
    SCHED --> API
    API --> N1
    API --> N2
    API --> N3
    
    SCHED -."ç›‘å¬æœªè°ƒåº¦Pod".-> API
    SCHED -."å†™å…¥è°ƒåº¦å†³ç­–".-> API
```

### 1.2 è°ƒåº¦å™¨çš„åŸºæœ¬å·¥ä½œæµç¨‹

è°ƒåº¦å™¨çš„å·¥ä½œæµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š

```mermaid
flowchart TD
    A["ç›‘å¬ Pod äº‹ä»¶"] --> B["è·å–é›†ç¾¤çŠ¶æ€"]
    B --> C["æ‰§è¡Œè°ƒåº¦ç®—æ³•"]
    C --> D["ç»‘å®šå†³ç­–"]
    
    A1["Watch API ç›‘å¬<br/>spec.nodeName ä¸ºç©ºçš„ Pod"]
    B1["è·å–èŠ‚ç‚¹ä¿¡æ¯<br/>æ”¶é›†èµ„æºä½¿ç”¨æƒ…å†µ<br/>è·å– Pod åˆ†å¸ƒ"]
    C1["é¢„é€‰é˜¶æ®µï¼šç­›é€‰èŠ‚ç‚¹<br/>ä¼˜é€‰é˜¶æ®µï¼šèŠ‚ç‚¹æ‰“åˆ†"]
    D1["å†™å…¥ API Server<br/>æ›´æ–° Pod.spec.nodeName"]
    
    A --- A1
    B --- B1
    C --- C1
    D --- D1
```

1. **ç›‘å¬ Pod äº‹ä»¶**
   - è°ƒåº¦å™¨é€šè¿‡ Watch API ç›‘å¬ API Server ä¸­ `spec.nodeName` ä¸ºç©ºçš„ Pod
   - è¿™äº› Pod è¢«è®¤ä¸ºæ˜¯å¾…è°ƒåº¦çš„ Pod

2. **è·å–é›†ç¾¤çŠ¶æ€**
   - è·å–æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹çš„ä¿¡æ¯
   - æ”¶é›†èŠ‚ç‚¹çš„èµ„æºä½¿ç”¨æƒ…å†µã€æ ‡ç­¾ã€æ±¡ç‚¹ç­‰ä¿¡æ¯
   - è·å–å·²è°ƒåº¦ Pod çš„åˆ†å¸ƒæƒ…å†µ

3. **æ‰§è¡Œè°ƒåº¦ç®—æ³•**
   - é¢„é€‰é˜¶æ®µï¼šç­›é€‰å‡ºæ»¡è¶³ Pod åŸºæœ¬è¦æ±‚çš„èŠ‚ç‚¹
   - ä¼˜é€‰é˜¶æ®µï¼šå¯¹é¢„é€‰èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†ï¼Œé€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹

4. **ç»‘å®šå†³ç­–**
   - å°†è°ƒåº¦å†³ç­–å†™å…¥ API Server
   - æ›´æ–° Pod çš„ `spec.nodeName` å­—æ®µ

### 1.3 é»˜è®¤è°ƒåº¦å™¨çš„è®¾è®¡åŸç†

é»˜è®¤è°ƒåº¦å™¨åŸºäºä»¥ä¸‹æ ¸å¿ƒè®¾è®¡åŸç†ï¼š

```mermaid
graph LR
    subgraph "è°ƒåº¦å™¨è®¾è®¡åŸç†"
        A["æ’ä»¶åŒ–æ¶æ„"]
        B["ä¸¤é˜¶æ®µè°ƒåº¦"]
        C["èµ„æºæ„ŸçŸ¥"]
    end
    
    A --> A1["æ”¯æŒæ‰©å±•å’Œè‡ªå®šä¹‰"]
    A --> A2["æ’ä»¶æ¥å£"]
    A --> A3["å¯ç”¨/ç¦ç”¨æ’ä»¶"]
    
    B --> B1["é¢„é€‰ï¼ˆFilteringï¼‰"]
    B --> B2["ä¼˜é€‰ï¼ˆScoringï¼‰"]
    
    C --> C1["CPUã€å†…å­˜ã€å­˜å‚¨"]
    C --> C2["æ‰©å±•èµ„æºï¼ˆGPUã€FPGAï¼‰"]
    C --> C3["èµ„æºåˆç†åˆ†é…"]
```

**æ’ä»¶åŒ–æ¶æ„ï¼š**

- è°ƒåº¦å™¨é‡‡ç”¨æ’ä»¶åŒ–è®¾è®¡ï¼Œæ”¯æŒæ‰©å±•å’Œè‡ªå®šä¹‰
- æ¯ä¸ªè°ƒåº¦é˜¶æ®µéƒ½æœ‰å¯¹åº”çš„æ’ä»¶æ¥å£
- æ”¯æŒå¯ç”¨/ç¦ç”¨ç‰¹å®šæ’ä»¶

**ä¸¤é˜¶æ®µè°ƒåº¦ï¼š**

- **è¿‡æ»¤ï¼ˆFilteringï¼‰**ï¼šè¿‡æ»¤ä¸æ»¡è¶³æ¡ä»¶çš„èŠ‚ç‚¹
- **è¯„åˆ†ï¼ˆScoringï¼‰**ï¼šå¯¹å€™é€‰èŠ‚ç‚¹è¿›è¡Œè¯„åˆ†æ’åº

**èµ„æºæ„ŸçŸ¥ï¼š**

- è€ƒè™‘ CPUã€å†…å­˜ã€å­˜å‚¨ç­‰èµ„æºéœ€æ±‚
- æ”¯æŒæ‰©å±•èµ„æºç±»å‹ï¼ˆå¦‚ GPUã€FPGAï¼‰
- å®ç°èµ„æºçš„åˆç†åˆ†é…å’Œåˆ©ç”¨

### 1.4 è°ƒåº¦ç†è®ºåŸºç¡€

#### 1.4.1 è°ƒåº¦é—®é¢˜çš„æ•°å­¦å»ºæ¨¡

è°ƒåº¦é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**çº¦æŸä¼˜åŒ–é—®é¢˜**ï¼Œå¯ä»¥ç”¨æ•°å­¦æ¨¡å‹è¡¨ç¤ºï¼š

**ç›®æ ‡å‡½æ•°ï¼š**

```text
minimize: f(x) = Î£(wi Ã— scorei(x))
å…¶ä¸­ï¼š
- x è¡¨ç¤ºè°ƒåº¦å†³ç­–å‘é‡
- wi è¡¨ç¤ºç¬¬iä¸ªè¯„åˆ†æ’ä»¶çš„æƒé‡
- scorei(x) è¡¨ç¤ºç¬¬iä¸ªæ’ä»¶çš„è¯„åˆ†å‡½æ•°
```

**çº¦æŸæ¡ä»¶ï¼š**

- **èµ„æºçº¦æŸ**ï¼šPod èµ„æºéœ€æ±‚ â‰¤ èŠ‚ç‚¹å¯ç”¨èµ„æº
- **äº²å’Œæ€§çº¦æŸ**ï¼šæ»¡è¶³èŠ‚ç‚¹å’Œ Pod äº²å’Œæ€§è§„åˆ™
- **åäº²å’Œæ€§çº¦æŸ**ï¼šæ»¡è¶³ Pod åˆ†æ•£éƒ¨ç½²è¦æ±‚
- **æ±¡ç‚¹å®¹å¿çº¦æŸ**ï¼šPod å¿…é¡»å®¹å¿èŠ‚ç‚¹æ±¡ç‚¹

#### 1.4.2 å¤šç›®æ ‡ä¼˜åŒ–ç†è®º

è°ƒåº¦å™¨éœ€è¦åŒæ—¶ä¼˜åŒ–å¤šä¸ªç›¸äº’å†²çªçš„ç›®æ ‡ï¼š

```mermaid
graph TB
    subgraph "è°ƒåº¦ä¼˜åŒ–ç›®æ ‡"
        A["èµ„æºåˆ©ç”¨ç‡æœ€å¤§åŒ–"]
        B["è´Ÿè½½å‡è¡¡"]
        C["æœåŠ¡è´¨é‡ä¿è¯"]
        D["è°ƒåº¦å»¶è¿Ÿæœ€å°åŒ–"]
    end
    
    A -."å†²çª".-> B
    B -."å†²çª".-> C
    C -."å†²çª".-> D
    D -."å†²çª".-> A
```

**å¸•ç´¯æ‰˜æœ€ä¼˜è§£ï¼š**

- è°ƒåº¦å†³ç­–é€šå¸¸æ— æ³•åŒæ—¶è¾¾åˆ°æ‰€æœ‰ç›®æ ‡çš„æœ€ä¼˜å€¼
- éœ€è¦åœ¨ä¸åŒç›®æ ‡é—´è¿›è¡Œæƒè¡¡
- é€šè¿‡åŠ æƒè¯„åˆ†å®ç°å¤šç›®æ ‡ä¼˜åŒ–

#### 1.4.3 è°ƒåº¦ç®—æ³•çš„è®¡ç®—å¤æ‚åº¦

**æ—¶é—´å¤æ‚åº¦åˆ†æï¼š**

- **è¿‡æ»¤é˜¶æ®µ**ï¼šO(N Ã— F)ï¼Œå…¶ä¸­ N æ˜¯èŠ‚ç‚¹æ•°ï¼ŒF æ˜¯è¿‡æ»¤æ’ä»¶æ•°
- **è¯„åˆ†é˜¶æ®µ**ï¼šO(M Ã— S)ï¼Œå…¶ä¸­ M æ˜¯å€™é€‰èŠ‚ç‚¹æ•°ï¼ˆM â‰¤ Nï¼‰ï¼ŒS æ˜¯è¯„åˆ†æ’ä»¶æ•°
- **æ€»ä½“å¤æ‚åº¦**ï¼šO(N Ã— F + M Ã— S)
- **æœ€åæƒ…å†µ**ï¼šå½“æ‰€æœ‰èŠ‚ç‚¹éƒ½é€šè¿‡è¿‡æ»¤æ—¶ï¼ŒM = Nï¼Œå¤æ‚åº¦ä¸º O(N Ã— (F + S))
- **æœ€ä½³æƒ…å†µ**ï¼šå½“å¤§éƒ¨åˆ†èŠ‚ç‚¹è¢«è¿‡æ»¤æ—¶ï¼ŒM << Nï¼Œå¤æ‚åº¦æ¥è¿‘ O(N Ã— F)

**ç©ºé—´å¤æ‚åº¦ï¼š**

- **èŠ‚ç‚¹ä¿¡æ¯ç¼“å­˜**ï¼šO(N Ã— K)ï¼Œå…¶ä¸­ K æ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„å¹³å‡ä¿¡æ¯é‡
- **Pod è°ƒåº¦çŠ¶æ€**ï¼šO(P Ã— L)ï¼Œå…¶ä¸­ P æ˜¯å¾…è°ƒåº¦ Pod æ•°ï¼ŒL æ˜¯è°ƒåº¦çŠ¶æ€ä¿¡æ¯é‡
- **æ’ä»¶çŠ¶æ€ç¼“å­˜**ï¼šO(N Ã— F + N Ã— S)ï¼Œå­˜å‚¨æ’ä»¶æ‰§è¡Œçš„ä¸­é—´ç»“æœ

**æ€§èƒ½ç“¶é¢ˆåˆ†æï¼š**

- **API Server äº¤äº’**ï¼šé¢‘ç¹çš„èŠ‚ç‚¹ä¿¡æ¯æŸ¥è¯¢
- **æ’ä»¶æ‰§è¡Œå¼€é”€**ï¼šå¤æ‚çš„è¿‡æ»¤å’Œè¯„åˆ†é€»è¾‘
- **å†…å­˜ä½¿ç”¨**ï¼šå¤§è§„æ¨¡é›†ç¾¤çš„èŠ‚ç‚¹ä¿¡æ¯ç¼“å­˜
- **è°ƒåº¦å»¶è¿Ÿ**ï¼šä¸²è¡Œå¤„ç†å¤šä¸ª Pod çš„è°ƒåº¦è¯·æ±‚

#### 1.4.4 è°ƒåº¦å™¨æ€§èƒ½ä¼˜åŒ–

**ç¼“å­˜æœºåˆ¶ï¼š**

```go
// èŠ‚ç‚¹ä¿¡æ¯ç¼“å­˜ç¤ºä¾‹
type NodeInfoCache struct {
    nodes map[string]*NodeInfo
    mutex sync.RWMutex
    
    // å¢é‡æ›´æ–°æœºåˆ¶
    generation int64
    lastUpdate time.Time
}

// ç¼“å­˜æ›´æ–°ç­–ç•¥
func (c *NodeInfoCache) UpdateNode(node *v1.Node) {
    c.mutex.Lock()
    defer c.mutex.Unlock()
    
    // åªæ›´æ–°å˜åŒ–çš„å­—æ®µ
    if existing, ok := c.nodes[node.Name]; ok {
        if existing.Generation == node.Generation {
            return // æ— éœ€æ›´æ–°
        }
    }
    
    c.nodes[node.Name] = NewNodeInfo(node)
    c.generation++
    c.lastUpdate = time.Now()
}
```

**å¹¶å‘ä¼˜åŒ–ï¼š**

- **è¿‡æ»¤æ’ä»¶å¹¶è¡Œæ‰§è¡Œ**ï¼šåˆ©ç”¨ goroutine å¹¶è¡Œè¿è¡Œå¤šä¸ªè¿‡æ»¤æ’ä»¶
- **è¯„åˆ†æ’ä»¶æ‰¹é‡å¤„ç†**ï¼šæ‰¹é‡è®¡ç®—å¤šä¸ªèŠ‚ç‚¹çš„è¯„åˆ†
- **è°ƒåº¦é˜Ÿåˆ—ä¼˜åŒ–**ï¼šä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—ç®¡ç†å¾…è°ƒåº¦ Pod
- **å¿«ç…§æœºåˆ¶**ï¼šåˆ›å»ºé›†ç¾¤çŠ¶æ€å¿«ç…§ï¼Œä¿è¯è°ƒåº¦å†³ç­–çš„ä¸€è‡´æ€§

**å†…å­˜ä¼˜åŒ–ï¼š**

```yaml
# è°ƒåº¦å™¨èµ„æºé…ç½®ä¼˜åŒ–
apiVersion: v1
kind: Pod
metadata:
  name: kube-scheduler
spec:
  containers:
  - name: kube-scheduler
    image: registry.k8s.io/kube-scheduler:v1.27.0
    resources:
      requests:
        cpu: "100m"
        memory: "256Mi"
      limits:
        cpu: "2000m"     # æ ¹æ®é›†ç¾¤è§„æ¨¡è°ƒæ•´
        memory: "2Gi"     # å¤§è§„æ¨¡é›†ç¾¤éœ€è¦æ›´å¤šå†…å­˜
    env:
    - name: GOMAXPROCS
      value: "4"          # é™åˆ¶ Go è¿è¡Œæ—¶ä½¿ç”¨çš„ CPU æ ¸å¿ƒæ•°
```

**è°ƒåº¦å»¶è¿Ÿä¼˜åŒ–ï¼š**

- **æ—©æœŸç»ˆæ­¢ç­–ç•¥**ï¼šä¸€æ—¦å‘ç°ä¸æ»¡è¶³æ¡ä»¶çš„èŠ‚ç‚¹ç«‹å³æ’é™¤
- **é¢„è®¡ç®—ä¼˜åŒ–**ï¼šé¢„å…ˆè®¡ç®—å¸¸ç”¨çš„è¯„åˆ†ä¿¡æ¯
- **å¢é‡è°ƒåº¦**ï¼šåªé‡æ–°è®¡ç®—å‘ç”Ÿå˜åŒ–çš„èŠ‚ç‚¹
- **è°ƒåº¦å™¨åˆ†ç‰‡**ï¼šä½¿ç”¨å¤šä¸ªè°ƒåº¦å™¨å®ä¾‹å¤„ç†ä¸åŒç±»å‹çš„å·¥ä½œè´Ÿè½½

---

## 2. è°ƒåº¦è¿‡ç¨‹è¯¦è§£

### 2.1 è°ƒåº¦æµæ°´çº¿ï¼šè¿‡æ»¤(Filtering)å’Œè¯„åˆ†(Scoring)

è°ƒåº¦æµæ°´çº¿æ˜¯è°ƒåº¦å™¨çš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ï¼Œåˆ†ä¸ºä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚

> **æœ¯è¯­è¯´æ˜**ï¼šåœ¨ Kubernetes æ—©æœŸç‰ˆæœ¬ä¸­ï¼Œè¿™ä¸¤ä¸ªé˜¶æ®µè¢«ç§°ä¸º Predicatesï¼ˆé¢„é€‰ï¼‰å’Œ Prioritiesï¼ˆä¼˜é€‰ï¼‰ã€‚ä» v1.19 å¼€å§‹ï¼Œå®˜æ–¹ç»Ÿä¸€ä½¿ç”¨ Filteringï¼ˆè¿‡æ»¤ï¼‰å’Œ Scoringï¼ˆè¯„åˆ†ï¼‰æœ¯è¯­ã€‚å½“å‰æ–‡æ¡£åŸºäº Kubernetes v1.27+ ç‰ˆæœ¬ï¼Œä½¿ç”¨ç¨³å®šçš„ v1 APIã€‚

```mermaid
flowchart LR
    subgraph "è°ƒåº¦æµæ°´çº¿"
        A["æ‰€æœ‰èŠ‚ç‚¹"] --> B["è¿‡æ»¤é˜¶æ®µ<br/>(Filtering)"]
        B --> C["å€™é€‰èŠ‚ç‚¹é›†åˆ"]
        C --> D["è¯„åˆ†é˜¶æ®µ<br/>(Scoring)"]
        D --> E["æœ€ä¼˜èŠ‚ç‚¹"]
    end
    
    subgraph "è¿‡æ»¤æ’ä»¶"
        B1["NodeResourcesFit"]
        B2["NodeAffinity"]
        B3["TaintToleration"]
        B4["PodTopologySpread"]
    end
    
    subgraph "è¯„åˆ†æ’ä»¶"
        D1["NodeResourcesFit"]
        D2["NodeAffinity"]
        D3["InterPodAffinity"]
    end
    
    B -.-> B1
    B -.-> B2
    B -.-> B3
    B -.-> B4
    
    D -.-> D1
    D -.-> D2
    D -.-> D3
```

#### 2.1.1 è¿‡æ»¤é˜¶æ®µï¼ˆFiltering Phaseï¼‰

è¿‡æ»¤é˜¶æ®µçš„ç›®æ ‡æ˜¯ç­›é€‰å‡ºèƒ½å¤Ÿè¿è¡Œ Pod çš„èŠ‚ç‚¹é›†åˆï¼š

- å¹¶è¡Œæ‰§è¡Œå¤šä¸ªè¿‡æ»¤æ’ä»¶
- ä»»ä½•ä¸€ä¸ªæ’ä»¶è¿”å›å¤±è´¥ï¼Œè¯¥èŠ‚ç‚¹è¢«æ’é™¤
- å¦‚æœæ²¡æœ‰èŠ‚ç‚¹é€šè¿‡è¿‡æ»¤ï¼ŒPod å°†ä¿æŒ Pending çŠ¶æ€

**è¿‡æ»¤åŸç†ï¼š**

- åŸºäºç¡¬çº¦æŸæ¡ä»¶è¿›è¡ŒèŠ‚ç‚¹ç­›é€‰
- é‡‡ç”¨"å¿«é€Ÿå¤±è´¥"ç­–ç•¥æé«˜æ•ˆç‡
- æ”¯æŒæ’ä»¶å¹¶è¡Œæ‰§è¡Œä»¥å‡å°‘è°ƒåº¦å»¶è¿Ÿ

#### 2.1.2 è¯„åˆ†é˜¶æ®µï¼ˆScoring Phaseï¼‰

è¯„åˆ†é˜¶æ®µå¯¹è¿‡æ»¤é€šè¿‡çš„èŠ‚ç‚¹è¿›è¡Œè¯„åˆ†ï¼š

- æ¯ä¸ªè¯„åˆ†æ’ä»¶ä¸ºèŠ‚ç‚¹æ‰“åˆ†ï¼ˆ0-100åˆ†ï¼‰
- è®¡ç®—åŠ æƒæ€»åˆ†
- é€‰æ‹©å¾—åˆ†æœ€é«˜çš„èŠ‚ç‚¹

**è¯„åˆ†åŸç†ï¼š**

- åŸºäºè½¯çº¦æŸæ¡ä»¶è¿›è¡ŒèŠ‚ç‚¹ä¼˜åŒ–é€‰æ‹©
- é€šè¿‡å¤šç»´åº¦è¯„åˆ†å®ç°è´Ÿè½½å‡è¡¡
- æ”¯æŒè‡ªå®šä¹‰æƒé‡è°ƒæ•´è°ƒåº¦åå¥½

### 2.2 å¸¸è§çš„è¿‡æ»¤ç­–ç•¥

#### 2.2.1 NodeResourcesFit

æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰è¶³å¤Ÿçš„èµ„æºæ»¡è¶³ Pod çš„éœ€æ±‚ï¼š

- CPU è¯·æ±‚é‡æ£€æŸ¥
- å†…å­˜è¯·æ±‚é‡æ£€æŸ¥
- å­˜å‚¨è¯·æ±‚é‡æ£€æŸ¥
- æ‰©å±•èµ„æºæ£€æŸ¥

#### 2.2.2 NodeAffinity

æ ¹æ®èŠ‚ç‚¹äº²å’Œæ€§è§„åˆ™è¿‡æ»¤èŠ‚ç‚¹ï¼š

```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/arch
            operator: In
            values:
            - amd64
```

#### 2.2.3 PodTopologySpread

ç¡®ä¿ Pod åœ¨æ‹“æ‰‘åŸŸä¸­çš„å‡åŒ€åˆ†å¸ƒï¼š

- å¯ç”¨åŒºåˆ†å¸ƒ
- èŠ‚ç‚¹åˆ†å¸ƒ
- è‡ªå®šä¹‰æ‹“æ‰‘åŸŸ

#### 2.2.4 TaintToleration

æ£€æŸ¥ Pod æ˜¯å¦èƒ½å®¹å¿èŠ‚ç‚¹çš„æ±¡ç‚¹ï¼š

```yaml
spec:
  tolerations:
  - key: "node-type"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
```

### 2.3 å¸¸è§çš„è¯„åˆ†ç­–ç•¥

#### 2.3.1 NodeResourcesFit

åŸºäºèµ„æºåˆ©ç”¨ç‡è¿›è¡Œè¯„åˆ†ï¼Œæ”¯æŒå¤šç§è¯„åˆ†ç®—æ³•ï¼š

- **LeastAllocated**ï¼šä¼˜å…ˆé€‰æ‹©èµ„æºä½¿ç”¨ç‡ä½çš„èŠ‚ç‚¹
  - è¯„åˆ†å…¬å¼ï¼š`score = (capacity - allocated) / capacity Ã— 100`
  - é€‚ç”¨äºè´Ÿè½½å‡è¡¡åœºæ™¯
- **MostAllocated**ï¼šä¼˜å…ˆé€‰æ‹©èµ„æºä½¿ç”¨ç‡é«˜çš„èŠ‚ç‚¹
  - è¯„åˆ†å…¬å¼ï¼š`score = allocated / capacity Ã— 100`
  - é€‚ç”¨äºèµ„æºæ•´åˆåœºæ™¯
- **RequestedToCapacityRatio**ï¼šåŸºäºè¯·æ±‚é‡ä¸å®¹é‡æ¯”ä¾‹è¯„åˆ†
  - æ”¯æŒè‡ªå®šä¹‰è¯„åˆ†æ›²çº¿
  - å¯é’ˆå¯¹ä¸åŒèµ„æºç±»å‹è®¾ç½®ä¸åŒæƒé‡

**èµ„æºè¯„åˆ†ç†è®ºï¼š**

- é€šè¿‡æ•°å­¦å‡½æ•°å°†èµ„æºä½¿ç”¨æƒ…å†µæ˜ å°„ä¸ºè¯„åˆ†
- æ”¯æŒå¤šç»´èµ„æºçš„åŠ æƒè®¡ç®—
- è€ƒè™‘èµ„æºç¢ç‰‡åŒ–å¯¹è°ƒåº¦æ•ˆç‡çš„å½±å“

#### 2.3.2 NodeAffinity

æ ¹æ®èŠ‚ç‚¹äº²å’Œæ€§åå¥½è¿›è¡Œè¯„åˆ†ï¼š

```yaml
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80
        preference:
          matchExpressions:
          - key: node-type
            operator: In
            values:
            - ssd
```

**è¯„åˆ†æœºåˆ¶ï¼š**

- æ»¡è¶³åå¥½æ¡ä»¶çš„èŠ‚ç‚¹è·å¾—å¯¹åº”æƒé‡çš„è¯„åˆ†
- æ”¯æŒå¤šä¸ªåå¥½æ¡ä»¶çš„ç»„åˆè¯„åˆ†
- æƒé‡èŒƒå›´ï¼š1-100

#### 2.3.3 InterPodAffinity

åŸºäº Pod é—´äº²å’Œæ€§è¿›è¡Œè¯„åˆ†ï¼š

- **Pod åäº²å’Œæ€§**ï¼šåŒä¸€åº”ç”¨çš„ Pod å€¾å‘äºåˆ†æ•£éƒ¨ç½²
  - æé«˜æœåŠ¡å¯ç”¨æ€§
  - é¿å…å•ç‚¹æ•…éšœ
- **Pod äº²å’Œæ€§**ï¼šç›¸å…³åº”ç”¨çš„ Pod å€¾å‘äºå°±è¿‘éƒ¨ç½²
  - å‡å°‘ç½‘ç»œå»¶è¿Ÿ
  - æé«˜æ•°æ®ä¼ è¾“æ•ˆç‡

**è°ƒåº¦å…¬å¹³æ€§åŸç†ï¼š**

- é€šè¿‡æ‹“æ‰‘åŸŸåˆ†æ•£ç¡®ä¿å·¥ä½œè´Ÿè½½å‡åŒ€åˆ†å¸ƒ
- è€ƒè™‘ç°æœ‰ Pod åˆ†å¸ƒå¯¹æ–° Pod è°ƒåº¦çš„å½±å“
- å¹³è¡¡æ€§èƒ½ä¼˜åŒ–ä¸é«˜å¯ç”¨æ€§éœ€æ±‚

### 2.4 èŠ‚ç‚¹äº²å’Œæ€§ä¸åäº²å’Œæ€§

```mermaid
graph TB
    subgraph "èŠ‚ç‚¹äº²å’Œæ€§"
        A["Node Affinity"]
        A --> A1["ç¡¬äº²å’Œæ€§<br/>(Required)"]
        A --> A2["è½¯äº²å’Œæ€§<br/>(Preferred)"]
    end
    
    subgraph "Pod äº²å’Œæ€§"
        B["Pod Affinity"]
        B --> B1["Pod äº²å’Œæ€§<br/>(å°±è¿‘éƒ¨ç½²)"]
        B --> B2["Pod åäº²å’Œæ€§<br/>(åˆ†æ•£éƒ¨ç½²)"]
    end
    
    A1 --> A1_DESC["å¿…é¡»æ»¡è¶³æ¡ä»¶<br/>å¦åˆ™ä¸è°ƒåº¦"]
    A2 --> A2_DESC["ä¼˜å…ˆæ»¡è¶³æ¡ä»¶<br/>ä½†ä¸å¼ºåˆ¶"]
    
    B1 --> B1_DESC["ç›¸å…³ Pod è°ƒåº¦åˆ°<br/>ç›¸è¿‘ä½ç½®"]
    B2 --> B2_DESC["é¿å… Pod è°ƒåº¦åˆ°<br/>åŒä¸€ä½ç½®"]
```

#### 2.4.1 èŠ‚ç‚¹äº²å’Œæ€§ï¼ˆNode Affinityï¼‰

èŠ‚ç‚¹äº²å’Œæ€§å…è®¸ Pod æŒ‡å®šå¯¹èŠ‚ç‚¹çš„åå¥½ï¼š

**ç¡¬äº²å’Œæ€§ï¼ˆRequiredï¼‰ï¼š**

```yaml
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
```

**è½¯äº²å’Œæ€§ï¼ˆPreferredï¼‰ï¼š**

```yaml
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values:
            - us-west-1a
```

### 2.5 Pod äº²å’Œæ€§ä¸åäº²å’Œæ€§

#### 2.5.1 Pod äº²å’Œæ€§ï¼ˆPod Affinityï¼‰

ä½¿ç›¸å…³çš„ Pod è°ƒåº¦åˆ°ç›¸è¿‘çš„ä½ç½®ï¼š

```yaml
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - database
        topologyKey: kubernetes.io/hostname
```

#### 2.5.2 Pod åäº²å’Œæ€§ï¼ˆPod Anti-Affinityï¼‰

ä½¿ Pod åˆ†æ•£éƒ¨ç½²ï¼Œé¿å…å•ç‚¹æ•…éšœï¼š

```yaml
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - web-server
        topologyKey: kubernetes.io/hostname
```

### 2.6 æ±¡ç‚¹(Taints)å’Œå®¹å¿(Tolerations)

æ±¡ç‚¹å’Œå®¹å¿æœºåˆ¶å®ç°äº†**èŠ‚ç‚¹é€‰æ‹©æ€§è°ƒåº¦**ï¼Œæ˜¯ Kubernetes ä¸­é‡è¦çš„è°ƒåº¦çº¦æŸæœºåˆ¶ã€‚

```mermaid
graph LR
    subgraph "æ±¡ç‚¹å’Œå®¹å¿æœºåˆ¶"
        A["èŠ‚ç‚¹æ±¡ç‚¹<br/>(Taints)"] --> B["é˜»æ­¢ Pod è°ƒåº¦"]
        C["Pod å®¹å¿<br/>(Tolerations)"] --> D["å…è®¸è°ƒåº¦åˆ°<br/>æœ‰æ±¡ç‚¹çš„èŠ‚ç‚¹"]
    end
    
    subgraph "æ±¡ç‚¹æ•ˆæœ"
        E["NoSchedule<br/>ä¸è°ƒåº¦æ–° Pod"]
        F["PreferNoSchedule<br/>å°½é‡ä¸è°ƒåº¦"]
        G["NoExecute<br/>é©±é€ç°æœ‰ Pod"]
    end
    
    B --> E
    B --> F
    B --> G
```

**è®¾è®¡åŸç†ï¼š**

- **é»˜è®¤æ‹’ç»ç­–ç•¥**ï¼šèŠ‚ç‚¹é»˜è®¤æ‹’ç»æ‰€æœ‰ä¸èƒ½å®¹å¿å…¶æ±¡ç‚¹çš„ Pod
- **ç²¾ç¡®åŒ¹é…æœºåˆ¶**ï¼šPod å¿…é¡»æ˜ç¡®å£°æ˜å¯¹ç‰¹å®šæ±¡ç‚¹çš„å®¹å¿
- **æ—¶é—´æ§åˆ¶**ï¼šæ”¯æŒåŸºäºæ—¶é—´çš„å®¹å¿ç­–ç•¥

#### 2.6.1 æ±¡ç‚¹ï¼ˆTaintsï¼‰

æ±¡ç‚¹ç”¨äºæ ‡è®°èŠ‚ç‚¹ï¼Œé˜»æ­¢ä¸åˆé€‚çš„ Pod è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ï¼š

```bash
# æ·»åŠ æ±¡ç‚¹
kubectl taint nodes node1 key1=value1:NoSchedule

# ç§»é™¤æ±¡ç‚¹
kubectl taint nodes node1 key1=value1:NoSchedule-
```

æ±¡ç‚¹æ•ˆæœç±»å‹ï¼š

- **NoSchedule**ï¼šä¸è°ƒåº¦æ–° Podï¼ˆç¡¬çº¦æŸï¼‰
- **PreferNoSchedule**ï¼šå°½é‡ä¸è°ƒåº¦æ–° Podï¼ˆè½¯çº¦æŸï¼‰
- **NoExecute**ï¼šé©±é€ç°æœ‰ Podï¼ˆè¿è¡Œæ—¶çº¦æŸï¼‰

**æ±¡ç‚¹åŒ¹é…ç®—æ³•ï¼š**

- æ±¡ç‚¹ç”± `key=value:effect` ä¸‰å…ƒç»„ç»„æˆ
- æ”¯æŒé€šé…ç¬¦åŒ¹é…å’Œç²¾ç¡®åŒ¹é…
- å¤šä¸ªæ±¡ç‚¹é‡‡ç”¨é€»è¾‘ AND å…³ç³»

#### 2.6.2 å®¹å¿ï¼ˆTolerationsï¼‰

Pod é€šè¿‡å®¹å¿æ¥"å®¹å¿"èŠ‚ç‚¹çš„æ±¡ç‚¹ï¼š

```yaml
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"    # ç²¾ç¡®åŒ¹é…
    value: "value1"
    effect: "NoSchedule"
  - key: "key2"
    operator: "Exists"   # å­˜åœ¨æ€§åŒ¹é…
    effect: "NoExecute"
    tolerationSeconds: 3600  # å®¹å¿æ—¶é—´
```

**å®¹å¿åŒ¹é…è§„åˆ™ï¼š**

- **Equal æ“ä½œç¬¦**ï¼škeyã€valueã€effect å¿…é¡»å®Œå…¨åŒ¹é…
- **Exists æ“ä½œç¬¦**ï¼šåªéœ€ key å’Œ effect åŒ¹é…
- **ç©º effect**ï¼šåŒ¹é…æ‰€æœ‰ effect ç±»å‹
- **tolerationSeconds**ï¼šä»…å¯¹ NoExecute æ•ˆæœæœ‰æ•ˆï¼ŒæŒ‡å®šå®¹å¿æ—¶é—´

---

## 3. è°ƒåº¦å™¨é…ç½®ä¸è‡ªå®šä¹‰

### 3.1 è°ƒåº¦é…ç½®æ–‡ä»¶ä»‹ç»

Kubernetes è°ƒåº¦å™¨æ”¯æŒé€šè¿‡é…ç½®æ–‡ä»¶è¿›è¡Œè‡ªå®šä¹‰é…ç½®ã€‚é…ç½®æ–‡ä»¶é‡‡ç”¨ YAML æ ¼å¼ï¼Œå…è®¸ç”¨æˆ·ï¼š

- å¯ç”¨/ç¦ç”¨ç‰¹å®šæ’ä»¶
- é…ç½®æ’ä»¶å‚æ•°
- è®¾ç½®è°ƒåº¦å™¨è¡Œä¸º

#### 3.1.1 åŸºæœ¬é…ç½®ç»“æ„

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: default-scheduler
  plugins:
    filter:
      enabled:
      - name: NodeResourcesFit
      - name: NodeAffinity
      disabled:
      - name: VolumeRestrictions
    score:
      enabled:
      - name: NodeResourcesFit
      - name: NodeAffinity
  pluginConfig:
  - name: NodeResourcesFit
    args:
      scoringStrategy:
        type: LeastAllocated
```

#### 3.1.2 å¸¸ç”¨é…ç½®é€‰é¡¹

**èµ„æºé…ç½®ï¼š**

```yaml
pluginConfig:
- name: NodeResourcesFit
  args:
    scoringStrategy:
      type: LeastAllocated
      resources:
      - name: cpu
        weight: 1
      - name: memory
        weight: 1
```

**äº²å’Œæ€§é…ç½®ï¼š**

```yaml
pluginConfig:
- name: InterPodAffinity
  args:
    hardPodAffinityWeight: 100
```

### 3.2 è°ƒåº¦æ’ä»¶æ¡†æ¶

è°ƒåº¦å™¨é‡‡ç”¨**æ’ä»¶åŒ–æ¶æ„**è®¾è®¡ï¼ŒåŸºäº**è´£ä»»é“¾æ¨¡å¼**å®ç°å¯æ‰©å±•çš„è°ƒåº¦é€»è¾‘ï¼š

**è®¾è®¡åŸç†ï¼š**

- **æ‰©å±•ç‚¹æœºåˆ¶**ï¼šåœ¨è°ƒåº¦æµç¨‹çš„å…³é”®èŠ‚ç‚¹æä¾›æ‰©å±•æ¥å£
- **æ’ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼šæ”¯æŒæ’ä»¶çš„åŠ¨æ€åŠ è½½å’Œé…ç½®
- **çŠ¶æ€ä¼ é€’**ï¼šé€šè¿‡ CycleState åœ¨æ’ä»¶é—´ä¼ é€’è°ƒåº¦çŠ¶æ€
- **å¹¶å‘å®‰å…¨**ï¼šç¡®ä¿å¤šä¸ªæ’ä»¶å¹¶å‘æ‰§è¡Œæ—¶çš„æ•°æ®ä¸€è‡´æ€§

```mermaid
flowchart TD
    A["Pod è°ƒåº¦å¼€å§‹"] --> B["PreFilter"]
    B --> C["Filter"]
    C --> D{"æœ‰å¯ç”¨èŠ‚ç‚¹?"}
    D -->|å¦| E["PostFilter"]
    D -->|æ˜¯| F["PreScore"]
    F --> G["Score"]
    G --> H["Reserve"]
    H --> I["Permit"]
    I --> J{"å…è®¸ç»‘å®š?"}
    J -->|å¦| K["ç­‰å¾…æˆ–æ‹’ç»"]
    J -->|æ˜¯| L["PreBind"]
    L --> M["Bind"]
    M --> N["PostBind"]
    N --> O["è°ƒåº¦å®Œæˆ"]
    E --> P["è°ƒåº¦å¤±è´¥"]
    K --> P
```

#### 3.2.1 æ’ä»¶æ‰©å±•ç‚¹è¯¦è§£

è°ƒåº¦å™¨æä¾› **10 ä¸ªæ‰©å±•ç‚¹**ï¼Œè¦†ç›–è°ƒåº¦çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚æ¯ä¸ªæ‰©å±•ç‚¹éƒ½æœ‰ç‰¹å®šçš„èŒè´£å’Œæ‰§è¡Œæ—¶æœºï¼š

**1. PreFilter æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šè¿‡æ»¤é˜¶æ®µä¹‹å‰
- **ä¸»è¦èŒè´£**ï¼š
  - é¢„è®¡ç®—ä¿¡æ¯æˆ–æ£€æŸ¥å‰ç½®æ¡ä»¶
  - æå‰ç»ˆæ­¢æ˜æ˜¾ä¸å¯è°ƒåº¦çš„ Pod
  - ä¸ºåç»­æ’ä»¶å‡†å¤‡å…±äº«çŠ¶æ€
- **æ€§èƒ½ä¼˜åŒ–**ï¼šæ”¯æŒçŠ¶æ€ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
- **å…¸å‹åº”ç”¨**ï¼šèµ„æºé¢„æ£€æŸ¥ã€æ‹“æ‰‘çº¦æŸé¢„å¤„ç†

**2. Filter æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šè¿‡æ»¤é˜¶æ®µçš„æ ¸å¿ƒç¯èŠ‚
- **ä¸»è¦èŒè´£**ï¼š
  - å®ç°ç¡¬çº¦æŸæ¡ä»¶æ£€æŸ¥ï¼ˆå¿…é¡»æ»¡è¶³ï¼‰
  - å†³å®šèŠ‚ç‚¹æ˜¯å¦é€‚åˆè¿è¡Œ Pod
  - è¿”å›æ˜ç¡®çš„é€šè¿‡/æ‹’ç»ç»“æœ
- **å¹¶å‘ç‰¹æ€§**ï¼šæ”¯æŒå¹¶è¡Œæ‰§è¡Œå¤šä¸ªè¿‡æ»¤æ’ä»¶
- **å¤±è´¥ç­–ç•¥**ï¼šä»»ä¸€æ’ä»¶å¤±è´¥åˆ™èŠ‚ç‚¹è¢«æ’é™¤
- **å…¸å‹æ’ä»¶**ï¼šNodeResourcesFitã€NodeAffinityã€TaintToleration

**3. PostFilter æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šå½“æ²¡æœ‰èŠ‚ç‚¹é€šè¿‡è¿‡æ»¤æ—¶
- **ä¸»è¦èŒè´£**ï¼š
  - å®ç°æŠ¢å é€»è¾‘ï¼ˆPreemptionï¼‰
  - æä¾›è°ƒåº¦å¤±è´¥çš„è¡¥æ•‘æªæ–½
  - å°è¯•ä¸º Pod åˆ›å»ºè°ƒåº¦æœºä¼š
- **æŠ¢å æœºåˆ¶**ï¼šå¯ä»¥é©±é€ä½ä¼˜å…ˆçº§ Pod ä¸ºé«˜ä¼˜å…ˆçº§ Pod è®©è·¯
- **å…¸å‹åº”ç”¨**ï¼šDefaultPreemption æ’ä»¶

**4. PreScore æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šè¯„åˆ†é˜¶æ®µä¹‹å‰
- **ä¸»è¦èŒè´£**ï¼š
  - ä¸ºè¯„åˆ†é˜¶æ®µå‡†å¤‡æ•°æ®å’ŒçŠ¶æ€
  - é¢„è®¡ç®—è¯„åˆ†æ‰€éœ€çš„å¤æ‚ä¿¡æ¯
  - ä¼˜åŒ–è¯„åˆ†é˜¶æ®µçš„æ•´ä½“æ€§èƒ½
- **æ•°æ®å…±äº«**ï¼šé€šè¿‡ CycleState ä¸ºè¯„åˆ†æ’ä»¶æä¾›é¢„è®¡ç®—ç»“æœ
- **å…¸å‹åº”ç”¨**ï¼šæ‹“æ‰‘ä¿¡æ¯æ”¶é›†ã€äº²å’Œæ€§å…³ç³»é¢„å¤„ç†

**5. Score æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šè¯„åˆ†é˜¶æ®µçš„æ ¸å¿ƒç¯èŠ‚
- **ä¸»è¦èŒè´£**ï¼š
  - ä¸ºæ¯ä¸ªå€™é€‰èŠ‚ç‚¹æ‰“åˆ†ï¼ˆ0-100åˆ†ï¼‰
  - å®ç°è½¯çº¦æŸæ¡ä»¶ï¼ˆåå¥½æ€§è¦æ±‚ï¼‰
  - æ”¯æŒå¤šç»´åº¦è¯„åˆ†ç­–ç•¥
- **è¯„åˆ†ç®—æ³•**ï¼šæ”¯æŒçº¿æ€§ã€æŒ‡æ•°ç­‰å¤šç§è¯„åˆ†å‡½æ•°
- **æƒé‡æœºåˆ¶**ï¼šå¯é…ç½®ä¸åŒæ’ä»¶çš„æƒé‡
- **å…¸å‹æ’ä»¶**ï¼šNodeResourcesFitã€NodeAffinityã€InterPodAffinity

**6. Reserve æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šé€‰å®šèŠ‚ç‚¹åï¼Œç»‘å®šä¹‹å‰
- **ä¸»è¦èŒè´£**ï¼š
  - ä¸º Pod åœ¨èŠ‚ç‚¹ä¸Šé¢„ç•™èµ„æº
  - é˜²æ­¢å¹¶å‘è°ƒåº¦å¯¼è‡´çš„èµ„æºç«äº‰
  - å®ç°äº‹åŠ¡æ€§çš„èµ„æºåˆ†é…
- **çŠ¶æ€ç®¡ç†**ï¼šç»´æŠ¤èµ„æºé¢„ç•™çŠ¶æ€ï¼Œæ”¯æŒå›æ»š
- **å¹¶å‘æ§åˆ¶**ï¼šç¡®ä¿å¤šä¸ªè°ƒåº¦å™¨çš„èµ„æºåˆ†é…ä¸€è‡´æ€§
- **å…¸å‹åº”ç”¨**ï¼šVolumeBindingã€NodeResourcesFit

**7. Permit æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šèµ„æºé¢„ç•™åï¼Œç»‘å®šä¹‹å‰
- **ä¸»è¦èŒè´£**ï¼š
  - å®ç°è°ƒåº¦é—¨æ§é€»è¾‘
  - æ”¯æŒå»¶è¿Ÿç»‘å®šæˆ–æ‹’ç»ç»‘å®š
  - åè°ƒæ‰¹é‡è°ƒåº¦ç­–ç•¥
- **æ§åˆ¶æœºåˆ¶**ï¼šå¯è¿”å› Allowã€Denyã€Wait ä¸‰ç§çŠ¶æ€
- **è¶…æ—¶å¤„ç†**ï¼šæ”¯æŒç­‰å¾…è¶…æ—¶åçš„è‡ªåŠ¨å¤„ç†
- **å…¸å‹åº”ç”¨**ï¼šCoschedulingï¼ˆååŒè°ƒåº¦ï¼‰ã€èµ„æºé…é¢æ§åˆ¶

**8. PreBind æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šç»‘å®šæ“ä½œä¹‹å‰
- **ä¸»è¦èŒè´£**ï¼š
  - æ‰§è¡Œç»‘å®šå‰çš„å¿…è¦å‡†å¤‡å·¥ä½œ
  - ç¡®ä¿èŠ‚ç‚¹å’Œèµ„æºçš„å°±ç»ªçŠ¶æ€
  - å¤„ç†å­˜å‚¨ã€ç½‘ç»œç­‰åŸºç¡€è®¾æ–½é…ç½®
- **å¼‚æ­¥æ“ä½œ**ï¼šæ”¯æŒè€—æ—¶çš„å‡†å¤‡æ“ä½œ
- **é”™è¯¯å¤„ç†**ï¼šå¤±è´¥æ—¶å¯ä»¥å›æ»šä¹‹å‰çš„æ“ä½œ
- **å…¸å‹åº”ç”¨**ï¼šVolumeBindingï¼ˆå­˜å‚¨å·æŒ‚è½½ï¼‰ã€ç½‘ç»œé…ç½®

**9. Bind æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šå®é™…ç»‘å®šæ“ä½œ
- **ä¸»è¦èŒè´£**ï¼š
  - å°† Pod ç»‘å®šåˆ°é€‰å®šçš„èŠ‚ç‚¹
  - æ›´æ–° API Server ä¸­çš„ Pod çŠ¶æ€
  - å®Œæˆè°ƒåº¦å†³ç­–çš„æœ€ç»ˆç¡®è®¤
- **é»˜è®¤å®ç°**ï¼šé€šå¸¸ç”± DefaultBinder æ’ä»¶å¤„ç†
- **è‡ªå®šä¹‰ç»‘å®š**ï¼šæ”¯æŒå®ç°ç‰¹æ®Šçš„ç»‘å®šé€»è¾‘
- **åŸå­æ“ä½œ**ï¼šç¡®ä¿ç»‘å®šæ“ä½œçš„åŸå­æ€§

**10. PostBind æ‰©å±•ç‚¹ï¼š**

- **æ‰§è¡Œæ—¶æœº**ï¼šç»‘å®šæ“ä½œå®Œæˆå
- **ä¸»è¦èŒè´£**ï¼š
  - æ‰§è¡Œç»‘å®šåçš„æ¸…ç†å’Œé€šçŸ¥å·¥ä½œ
  - æ›´æ–°ç¼“å­˜å’Œç»Ÿè®¡ä¿¡æ¯
  - è§¦å‘åç»­çš„è‡ªåŠ¨åŒ–æµç¨‹
- **å¼‚æ­¥æ‰§è¡Œ**ï¼šä¸å½±å“è°ƒåº¦ä¸»æµç¨‹çš„æ€§èƒ½
- **ç›‘æ§é›†æˆ**ï¼šæ›´æ–°è°ƒåº¦ç›¸å…³çš„ç›‘æ§æŒ‡æ ‡
- **å…¸å‹åº”ç”¨**ï¼šç¼“å­˜æ›´æ–°ã€äº‹ä»¶è®°å½•ã€ç›‘æ§æ•°æ®ä¸ŠæŠ¥

**æ‰©å±•ç‚¹æ‰§è¡Œé¡ºåºå’ŒçŠ¶æ€ä¼ é€’ï¼š**

```go
// CycleState ç”¨äºåœ¨æ‰©å±•ç‚¹ä¹‹é—´ä¼ é€’çŠ¶æ€
type CycleState struct {
    mx      sync.RWMutex
    storage map[StateKey]StateData
    
    // è®°å½•è°ƒåº¦è¿‡ç¨‹ä¸­çš„å…³é”®ä¿¡æ¯
    recordedPluginNominations map[string]*NominatedNode
}

// çŠ¶æ€æ•°æ®æ¥å£
type StateData interface {
    Clone() StateData
}

// æ’ä»¶çŠ¶æ€ç¤ºä¾‹
type NodeResourcesFitState struct {
    InsufficientResource []InsufficientResource
}

func (s *NodeResourcesFitState) Clone() StateData {
    return &NodeResourcesFitState{
        InsufficientResource: s.InsufficientResource,
    }
}
```

#### 3.2.2 è‡ªå®šä¹‰æ’ä»¶å¼€å‘

å¼€å‘è‡ªå®šä¹‰æ’ä»¶éœ€è¦å®ç°ç›¸åº”çš„æ¥å£ã€‚ä»¥ä¸‹ç¤ºä¾‹é€‚ç”¨äº Kubernetes v1.27+ï¼š

**åŸºç¡€æ¥å£å®šä¹‰ï¼š**

```go
// é€‚ç”¨äº Kubernetes v1.27+
package main

import (
    "context"
    "fmt"
    "time"
    
    v1 "k8s.io/api/core/v1"
    "k8s.io/kubernetes/pkg/scheduler/framework"
)

// è¿‡æ»¤æ’ä»¶æ¥å£
type FilterPlugin interface {
    framework.Plugin
    Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status
}

// è¯„åˆ†æ’ä»¶æ¥å£
type ScorePlugin interface {
    framework.Plugin
    Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status)
}
```

**è‡ªå®šä¹‰æ’ä»¶å®ç°ç¤ºä¾‹ï¼š**

```go
// CustomResourcePlugin è‡ªå®šä¹‰èµ„æºæ’ä»¶
type CustomResourcePlugin struct {
    handle framework.Handle
}

// Name è¿”å›æ’ä»¶åç§°
func (pl *CustomResourcePlugin) Name() string {
    return "CustomResourcePlugin"
}

// Filter å®ç°è¿‡æ»¤é€»è¾‘
func (pl *CustomResourcePlugin) Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
    // å‚æ•°éªŒè¯
    if nodeInfo == nil {
        return framework.NewStatus(framework.Error, "nodeInfo is nil")
    }
    if nodeInfo.Node() == nil {
        return framework.NewStatus(framework.Error, "node is nil")
    }
    
    // æ·»åŠ è¶…æ—¶æ§åˆ¶
    ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
    defer cancel()
    
    // æ£€æŸ¥è‡ªå®šä¹‰èµ„æº
    customResource := nodeInfo.Node().Status.Allocatable["example.com/custom-resource"]
    if customResource.IsZero() {
        return framework.NewStatus(framework.Unschedulable, "insufficient custom resource")
    }
    
    // æ£€æŸ¥ Pod æ˜¯å¦éœ€è¦è‡ªå®šä¹‰èµ„æº
    for _, container := range pod.Spec.Containers {
        if requested := container.Resources.Requests["example.com/custom-resource"]; !requested.IsZero() {
            if customResource.Cmp(requested) < 0 {
                return framework.NewStatus(framework.Unschedulable, 
                    fmt.Sprintf("insufficient custom resource: requested %v, available %v", 
                        requested, customResource))
            }
        }
    }
    
    return framework.NewStatus(framework.Success, "")
}

// Score å®ç°è¯„åˆ†é€»è¾‘
func (pl *CustomResourcePlugin) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
    // æ·»åŠ è¶…æ—¶æ§åˆ¶
    ctx, cancel := context.WithTimeout(ctx, 3*time.Second)
    defer cancel()
    
    nodeInfo, err := pl.handle.SnapshotSharedLister().NodeInfos().Get(nodeName)
    if err != nil {
        return 0, framework.NewStatus(framework.Error, fmt.Sprintf("getting node %q from Snapshot: %v", nodeName, err))
    }
    
    // åŸºäºå¯ç”¨èµ„æºæ¯”ä¾‹è¯„åˆ†
    node := nodeInfo.Node()
    if node == nil {
        return 0, framework.NewStatus(framework.Error, "node not found")
    }
    
    allocatable := node.Status.Allocatable["example.com/custom-resource"]
    if allocatable.IsZero() {
        return 0, framework.NewStatus(framework.Success, "")
    }
    
    // è®¡ç®—èµ„æºåˆ©ç”¨ç‡å¹¶è½¬æ¢ä¸ºè¯„åˆ† (0-100)
    used := nodeInfo.Requested.ScalarResources["example.com/custom-resource"]
    utilization := float64(used) / float64(allocatable.Value())
    
    // åå¥½èµ„æºåˆ©ç”¨ç‡è¾ƒä½çš„èŠ‚ç‚¹
    score := int64((1.0 - utilization) * 100)
    if score < 0 {
        score = 0
    }
    if score > 100 {
        score = 100
    }
    
    return score, framework.NewStatus(framework.Success, "")
}

// æ’ä»¶å·¥å‚å‡½æ•°
func New(obj runtime.Object, h framework.Handle) (framework.Plugin, error) {
    return &CustomResourcePlugin{
        handle: h,
    }, nil
}
```

**æ’ä»¶æ³¨å†Œé…ç½®ï¼š**

```yaml
# é€‚ç”¨äº Kubernetes v1.27+
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: custom-scheduler
  plugins:
    filter:
      enabled:
      - name: CustomResourcePlugin
    score:
      enabled:
      - name: CustomResourcePlugin
        weight: 10
  pluginConfig:
  - name: CustomResourcePlugin
    args:
      # æ’ä»¶ç‰¹å®šé…ç½®
      resourceName: "example.com/custom-resource"
      scoringStrategy: "LeastAllocated"
```

### 3.3 å¤šè°ƒåº¦å™¨éƒ¨ç½²

Kubernetes æ”¯æŒ**å¤šè°ƒåº¦å™¨å¹¶è¡Œè¿è¡Œ**ï¼Œå®ç°è°ƒåº¦ç­–ç•¥çš„å·®å¼‚åŒ–å’Œä¸“ä¸šåŒ–ï¼š

**è®¾è®¡åŸç†ï¼š**

- **è°ƒåº¦å™¨éš”ç¦»**ï¼šæ¯ä¸ªè°ƒåº¦å™¨ç‹¬ç«‹è¿è¡Œï¼Œé¿å…ç›¸äº’å¹²æ‰°
- **èµ„æºç«äº‰å¤„ç†**ï¼šé€šè¿‡ä¹è§‚é”æœºåˆ¶å¤„ç†å¤šè°ƒåº¦å™¨çš„èµ„æºç«äº‰
- **è´Ÿè½½åˆ†æ‹…**ï¼šä¸åŒç±»å‹çš„å·¥ä½œè´Ÿè½½ä½¿ç”¨ä¸“é—¨çš„è°ƒåº¦å™¨
- **æ•…éšœéš”ç¦»**ï¼šå•ä¸ªè°ƒåº¦å™¨æ•…éšœä¸å½±å“å…¶ä»–è°ƒåº¦å™¨

```mermaid
graph TB
    subgraph "å¤šè°ƒåº¦å™¨æ¶æ„"
        API["API Server"]
        
        subgraph "è°ƒåº¦å™¨å®ä¾‹"
            S1["é»˜è®¤è°ƒåº¦å™¨<br/>default-scheduler"]
            S2["GPU è°ƒåº¦å™¨<br/>gpu-scheduler"]
            S3["æ‰¹å¤„ç†è°ƒåº¦å™¨<br/>batch-scheduler"]
        end
        
        subgraph "Pod ç±»å‹"
            P1["é€šç”¨ Pod"]
            P2["GPU Pod"]
            P3["æ‰¹å¤„ç† Pod"]
        end
    end
    
    API --> S1
    API --> S2
    API --> S3
    
    P1 -."schedulerName: default-scheduler".-> S1
    P2 -."schedulerName: gpu-scheduler".-> S2
    P3 -."schedulerName: batch-scheduler".-> S3
```

#### 3.3.1 éƒ¨ç½²é¢å¤–è°ƒåº¦å™¨

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-scheduler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: custom-scheduler
  template:
    metadata:
      labels:
        app: custom-scheduler
    spec:
      containers:
      - name: kube-scheduler
        image: registry.k8s.io/kube-scheduler:v1.27.0
        command:
        - kube-scheduler
        - --config=/etc/kubernetes/scheduler-config.yaml
        - --v=2
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
        volumeMounts:
        - name: config
          mountPath: /etc/kubernetes
      volumes:
      - name: config
        configMap:
          name: scheduler-config
```

#### 3.3.2 æŒ‡å®šè°ƒåº¦å™¨

Pod å¯ä»¥é€šè¿‡ `schedulerName` å­—æ®µæŒ‡å®šä½¿ç”¨çš„è°ƒåº¦å™¨ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  schedulerName: gpu-scheduler  # æŒ‡å®šè‡ªå®šä¹‰è°ƒåº¦å™¨
  containers:
  - name: gpu-container
    image: tensorflow/tensorflow:latest-gpu
    resources:
      limits:
        nvidia.com/gpu: 1
```

#### 3.3.3 è°ƒåº¦å™¨é€‰æ‹©ç­–ç•¥

**æŒ‰å·¥ä½œè´Ÿè½½ç±»å‹åˆ†é…ï¼š**

- **é»˜è®¤è°ƒåº¦å™¨**ï¼šå¤„ç†é€šç”¨å·¥ä½œè´Ÿè½½
- **GPU è°ƒåº¦å™¨**ï¼šä¸“é—¨å¤„ç† GPU å¯†é›†å‹ä»»åŠ¡
- **æ‰¹å¤„ç†è°ƒåº¦å™¨**ï¼šä¼˜åŒ–æ‰¹å¤„ç†ä½œä¸šçš„è°ƒåº¦
- **å®æ—¶è°ƒåº¦å™¨**ï¼šå¤„ç†å¯¹å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨

**è°ƒåº¦å™¨è´Ÿè½½å‡è¡¡ï¼š**

```yaml
# ä½¿ç”¨ leader election å®ç°è°ƒåº¦å™¨é«˜å¯ç”¨
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-config
data:
  config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    leaderElection:
      leaderElect: true
      resourceName: custom-scheduler
      resourceNamespace: kube-system
    profiles:
    - schedulerName: custom-scheduler
      plugins:
        filter:
          enabled:
          - name: NodeResourcesFit
          - name: NodeAffinity
```

**è°ƒåº¦å™¨åˆ†ç±»ï¼š**

- **é»˜è®¤è°ƒåº¦å™¨**ï¼šé€‚ç”¨äºå¤§å¤šæ•°é€šç”¨åœºæ™¯
  - å¹³è¡¡æ€§èƒ½å’Œèµ„æºåˆ©ç”¨ç‡
  - æ”¯æŒæ ‡å‡†çš„è°ƒåº¦ç­–ç•¥
- **ä¸“ç”¨è°ƒåº¦å™¨**ï¼šé’ˆå¯¹ç‰¹å®šå·¥ä½œè´Ÿè½½ä¼˜åŒ–
  - GPU è°ƒåº¦å™¨ï¼šä¼˜åŒ– GPU èµ„æºåˆ†é…
  - æ‰¹å¤„ç†è°ƒåº¦å™¨ï¼šæ”¯æŒä½œä¸šé˜Ÿåˆ—å’Œä¼˜å…ˆçº§
  - å®æ—¶è°ƒåº¦å™¨ï¼šä¿è¯å»¶è¿Ÿæ•æ„Ÿåº”ç”¨çš„è°ƒåº¦
- **å¤šç§Ÿæˆ·è°ƒåº¦å™¨**ï¼šä¸ºä¸åŒç§Ÿæˆ·æä¾›éš”ç¦»çš„è°ƒåº¦ç­–ç•¥
  - èµ„æºé…é¢éš”ç¦»
  - è°ƒåº¦ç­–ç•¥å®šåˆ¶

**å¤šè°ƒåº¦å™¨ä¼˜åŠ¿ï¼š**

- **ç­–ç•¥ä¸“ä¸šåŒ–**ï¼šä¸åŒåº”ç”¨ä½¿ç”¨æœ€é€‚åˆçš„è°ƒåº¦ç­–ç•¥
- **èµ„æºä¸“é—¨åŒ–**ï¼šç‰¹æ®Šèµ„æºï¼ˆå¦‚ GPUã€FPGAï¼‰çš„ä¸“é—¨è°ƒåº¦
- **ç§Ÿæˆ·éš”ç¦»**ï¼šå¤šç§Ÿæˆ·ç¯å¢ƒä¸‹çš„è°ƒåº¦éš”ç¦»å’Œå®‰å…¨
- **æ¸è¿›å¼å‡çº§**ï¼šè°ƒåº¦å™¨çš„ç°åº¦å‡çº§å’Œ A/B æµ‹è¯•
- **æ•…éšœå®¹é”™**ï¼šå•ç‚¹æ•…éšœä¸å½±å“æ•´ä½“è°ƒåº¦èƒ½åŠ›

---

## 4. è°ƒåº¦å†³ç­–å®¡è®¡ä¸ç›‘æ§

### 4.1 è°ƒåº¦å†³ç­–å®¡è®¡æœºåˆ¶

è°ƒåº¦å†³ç­–å®¡è®¡æ˜¯ä¼ä¸šçº§ Kubernetes é›†ç¾¤çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç”¨äºè®°å½•ã€åˆ†æå’Œè¿½è¸ªè°ƒåº¦å™¨çš„å†³ç­–è¿‡ç¨‹ã€‚

#### 4.1.1 å®¡è®¡æ—¥å¿—é…ç½®

**å¯ç”¨è°ƒåº¦å™¨å®¡è®¡æ—¥å¿—ï¼š**

```yaml
# è°ƒåº¦å™¨å®¡è®¡ç­–ç•¥é…ç½®
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# è®°å½•æ‰€æœ‰è°ƒåº¦å†³ç­–
- level: Metadata
  namespaces: ["default", "production"]
  resources:
  - group: ""
    resources: ["pods"]
  verbs: ["create", "update", "patch"]
  omitStages: ["RequestReceived"]
  
# è®°å½•è°ƒåº¦å™¨ç»‘å®šæ“ä½œ
- level: Request
  resources:
  - group: ""
    resources: ["pods/binding"]
  verbs: ["create"]
  
# è®°å½•è°ƒåº¦å¤±è´¥äº‹ä»¶
- level: RequestResponse
  resources:
  - group: ""
    resources: ["events"]
  verbs: ["create"]
  namespaceSelector:
    matchLabels:
      audit: "scheduler"
```

**è°ƒåº¦å™¨éƒ¨ç½²é…ç½®ï¼š**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-scheduler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: custom-scheduler
  template:
    metadata:
      labels:
        app: custom-scheduler
    spec:
      containers:
      - name: kube-scheduler
        image: registry.k8s.io/kube-scheduler:v1.27.0
        command:
        - kube-scheduler
        - --config=/etc/kubernetes/scheduler-config.yaml
        - --audit-log-path=/var/log/scheduler-audit.log
        - --audit-log-maxage=30
        - --audit-log-maxbackup=10
        - --audit-log-maxsize=100
        - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
        - --v=2
        volumeMounts:
        - name: config
          mountPath: /etc/kubernetes
        - name: audit-logs
          mountPath: /var/log
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "2000m"
            memory: "2Gi"
      volumes:
      - name: config
        configMap:
          name: scheduler-config
      - name: audit-logs
        hostPath:
          path: /var/log/scheduler
          type: DirectoryOrCreate
```

#### 4.1.2 è°ƒåº¦äº‹ä»¶è¿½è¸ª

**è°ƒåº¦äº‹ä»¶åˆ†æè„šæœ¬ï¼š**

```bash
#!/bin/bash
# scheduler-audit-analyzer.sh

# åˆ†æè°ƒåº¦æˆåŠŸç‡
analyze_scheduling_success_rate() {
    local namespace=${1:-"default"}
    local time_range=${2:-"1h"}
    
    echo "=== è°ƒåº¦æˆåŠŸç‡åˆ†æ (æœ€è¿‘ $time_range) ==="
    
    # è·å–è°ƒåº¦äº‹ä»¶
    kubectl get events -n $namespace \
        --field-selector reason=Scheduled \
        --sort-by='.lastTimestamp' \
        --output=json | jq -r '
        .items[] | 
        select(.lastTimestamp > (now - 3600)) |
        "\(.lastTimestamp) \(.involvedObject.name) \(.message)"
    '
    
    # ç»Ÿè®¡è°ƒåº¦å¤±è´¥
    kubectl get events -n $namespace \
        --field-selector reason=FailedScheduling \
        --sort-by='.lastTimestamp' \
        --output=json | jq -r '
        .items[] | 
        select(.lastTimestamp > (now - 3600)) |
        "FAILED: \(.lastTimestamp) \(.involvedObject.name) \(.message)"
    '
}

# åˆ†æè°ƒåº¦å»¶è¿Ÿ
analyze_scheduling_latency() {
    echo "=== è°ƒåº¦å»¶è¿Ÿåˆ†æ ==="
    
    kubectl get pods --all-namespaces -o json | jq -r '
    .items[] |
    select(.status.conditions[]? | select(.type == "PodScheduled" and .status == "True")) |
    {
        name: .metadata.name,
        namespace: .metadata.namespace,
        created: .metadata.creationTimestamp,
        scheduled: (.status.conditions[] | select(.type == "PodScheduled").lastTransitionTime)
    } |
    "\(.namespace)/\(.name): \(.created) -> \(.scheduled)"
    '
}

# åˆ†æèŠ‚ç‚¹èµ„æºä½¿ç”¨
analyze_node_utilization() {
    echo "=== èŠ‚ç‚¹èµ„æºä½¿ç”¨åˆ†æ ==="
    
    kubectl top nodes --sort-by=cpu
    echo ""
    kubectl top nodes --sort-by=memory
}

# ä¸»å‡½æ•°
main() {
    case "$1" in
        "success-rate")
            analyze_scheduling_success_rate $2 $3
            ;;
        "latency")
            analyze_scheduling_latency
            ;;
        "utilization")
            analyze_node_utilization
            ;;
        "all")
            analyze_scheduling_success_rate
            echo ""
            analyze_scheduling_latency
            echo ""
            analyze_node_utilization
            ;;
        *)
            echo "ç”¨æ³•: $0 {success-rate|latency|utilization|all} [namespace] [time-range]"
            echo "ç¤ºä¾‹: $0 success-rate default 2h"
            exit 1
            ;;
    esac
}

main "$@"
```

#### 4.1.3 è°ƒåº¦æŒ‡æ ‡ç›‘æ§

**Prometheus ç›‘æ§é…ç½®ï¼š**

```yaml
# scheduler-monitoring.yaml
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: scheduler-metrics
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: custom-scheduler
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
---
apiVersion: v1
kind: Service
metadata:
  name: scheduler-metrics
  namespace: kube-system
  labels:
    app: custom-scheduler
spec:
  ports:
  - name: metrics
    port: 10259
    targetPort: 10259
  selector:
    app: custom-scheduler
```

**å…³é”®è°ƒåº¦æŒ‡æ ‡ï¼š**

```promql
# è°ƒåº¦å»¶è¿Ÿ P99
histogram_quantile(0.99, 
  rate(scheduler_scheduling_duration_seconds_bucket[5m])
)

# è°ƒåº¦æˆåŠŸç‡
rate(scheduler_pod_scheduling_attempts_total{result="scheduled"}[5m]) /
rate(scheduler_pod_scheduling_attempts_total[5m])

# å¾…è°ƒåº¦ Pod æ•°é‡
scheduler_pending_pods

# è°ƒåº¦å™¨é˜Ÿåˆ—é•¿åº¦
scheduler_queue_incoming_pods_total

# èŠ‚ç‚¹è¯„åˆ†è€—æ—¶
histogram_quantile(0.95,
  rate(scheduler_framework_extension_point_duration_seconds_bucket{
    extension_point="Score"
  }[5m])
)
```

### 4.2 é«˜çº§æ•…éšœæ’æŸ¥

#### 4.2.1 è°ƒåº¦å™¨æ€§èƒ½åˆ†æ

**ä½¿ç”¨ pprof åˆ†æè°ƒåº¦å™¨æ€§èƒ½ï¼š**

```bash
#!/bin/bash
# scheduler-profiling.sh

# è·å–è°ƒåº¦å™¨ Pod åç§°
SCHEDULER_POD=$(kubectl get pods -n kube-system -l component=kube-scheduler -o jsonpath='{.items[0].metadata.name}')

echo "è°ƒåº¦å™¨ Pod: $SCHEDULER_POD"

# ç«¯å£è½¬å‘
echo "å¯åŠ¨ç«¯å£è½¬å‘..."
kubectl port-forward -n kube-system pod/$SCHEDULER_POD 10259:10259 &
PORT_FORWARD_PID=$!

# ç­‰å¾…ç«¯å£è½¬å‘å°±ç»ª
sleep 3

# CPU æ€§èƒ½åˆ†æ
echo "æ”¶é›† CPU æ€§èƒ½æ•°æ® (30ç§’)..."
curl -s "http://localhost:10259/debug/pprof/profile?seconds=30" > scheduler-cpu.prof

# å†…å­˜æ€§èƒ½åˆ†æ
echo "æ”¶é›†å†…å­˜æ€§èƒ½æ•°æ®..."
curl -s "http://localhost:10259/debug/pprof/heap" > scheduler-heap.prof

# Goroutine åˆ†æ
echo "æ”¶é›† Goroutine æ•°æ®..."
curl -s "http://localhost:10259/debug/pprof/goroutine" > scheduler-goroutine.prof

# åœæ­¢ç«¯å£è½¬å‘
kill $PORT_FORWARD_PID

echo "æ€§èƒ½æ•°æ®æ”¶é›†å®Œæˆ:"
echo "  - CPU: scheduler-cpu.prof"
echo "  - å†…å­˜: scheduler-heap.prof"
echo "  - Goroutine: scheduler-goroutine.prof"
echo ""
echo "åˆ†æå‘½ä»¤:"
echo "  go tool pprof scheduler-cpu.prof"
echo "  go tool pprof scheduler-heap.prof"
echo "  go tool pprof scheduler-goroutine.prof"
```

**æ€§èƒ½åˆ†æç¤ºä¾‹ï¼š**

```bash
# åˆ†æ CPU çƒ­ç‚¹
go tool pprof scheduler-cpu.prof
(pprof) top 10
(pprof) list <function_name>
(pprof) web

# åˆ†æå†…å­˜ä½¿ç”¨
go tool pprof scheduler-heap.prof
(pprof) top 10 -cum
(pprof) list <function_name>

# åˆ†æ Goroutine æ³„æ¼
go tool pprof scheduler-goroutine.prof
(pprof) top 10
(pprof) traces
```

#### 4.2.2 è°ƒåº¦å¤±è´¥æ·±åº¦åˆ†æ

**è°ƒåº¦å¤±è´¥è¯Šæ–­å·¥å…·ï¼š**

```bash
#!/bin/bash
# scheduler-debug.sh

# è¯Šæ–­ç‰¹å®š Pod çš„è°ƒåº¦å¤±è´¥
debug_pod_scheduling() {
    local pod_name=$1
    local namespace=${2:-"default"}
    
    echo "=== è¯Šæ–­ Pod: $namespace/$pod_name ==="
    
    # 1. Pod åŸºæœ¬ä¿¡æ¯
    echo "1. Pod çŠ¶æ€:"
    kubectl get pod $pod_name -n $namespace -o wide
    echo ""
    
    # 2. Pod äº‹ä»¶
    echo "2. Pod äº‹ä»¶:"
    kubectl describe pod $pod_name -n $namespace | grep -A 20 "Events:"
    echo ""
    
    # 3. èµ„æºéœ€æ±‚åˆ†æ
    echo "3. èµ„æºéœ€æ±‚:"
    kubectl get pod $pod_name -n $namespace -o jsonpath='
    CPU è¯·æ±‚: {.spec.containers[*].resources.requests.cpu}
    å†…å­˜è¯·æ±‚: {.spec.containers[*].resources.requests.memory}
    CPU é™åˆ¶: {.spec.containers[*].resources.limits.cpu}
    å†…å­˜é™åˆ¶: {.spec.containers[*].resources.limits.memory}
    '
    echo ""
    
    # 4. èŠ‚ç‚¹é€‰æ‹©å™¨å’Œäº²å’Œæ€§
    echo "4. è°ƒåº¦çº¦æŸ:"
    kubectl get pod $pod_name -n $namespace -o yaml | grep -A 10 -E "(nodeSelector|affinity|tolerations)"
    echo ""
    
    # 5. å¯ç”¨èŠ‚ç‚¹åˆ†æ
    echo "5. èŠ‚ç‚¹èµ„æºçŠ¶æ€:"
    kubectl top nodes
    echo ""
    
    # 6. èŠ‚ç‚¹æ±¡ç‚¹æ£€æŸ¥
    echo "6. èŠ‚ç‚¹æ±¡ç‚¹:"
    kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints
    echo ""
    
    # 7. å­˜å‚¨å·æ£€æŸ¥
    echo "7. å­˜å‚¨å·çŠ¶æ€:"
    kubectl get pv,pvc -n $namespace
    echo ""
}

# é›†ç¾¤è°ƒåº¦å¥åº·æ£€æŸ¥
cluster_scheduling_health() {
    echo "=== é›†ç¾¤è°ƒåº¦å¥åº·æ£€æŸ¥ ==="
    
    # 1. è°ƒåº¦å™¨çŠ¶æ€
    echo "1. è°ƒåº¦å™¨çŠ¶æ€:"
    kubectl get pods -n kube-system -l component=kube-scheduler
    echo ""
    
    # 2. èŠ‚ç‚¹çŠ¶æ€
    echo "2. èŠ‚ç‚¹çŠ¶æ€:"
    kubectl get nodes -o wide
    echo ""
    
    # 3. å¾…è°ƒåº¦ Pod
    echo "3. å¾…è°ƒåº¦ Pod:"
    kubectl get pods --all-namespaces --field-selector=status.phase=Pending
    echo ""
    
    # 4. èµ„æºé…é¢
    echo "4. èµ„æºé…é¢ä½¿ç”¨:"
    kubectl get resourcequota --all-namespaces
    echo ""
    
    # 5. æœ€è¿‘è°ƒåº¦äº‹ä»¶
    echo "5. æœ€è¿‘è°ƒåº¦äº‹ä»¶:"
    kubectl get events --all-namespaces --sort-by='.lastTimestamp' | \
        grep -E "(Scheduled|FailedScheduling)" | tail -10
    echo ""
}

# è°ƒåº¦æ€§èƒ½åˆ†æ
scheduling_performance_analysis() {
    echo "=== è°ƒåº¦æ€§èƒ½åˆ†æ ==="
    
    # 1. è°ƒåº¦å»¶è¿Ÿç»Ÿè®¡
    echo "1. è°ƒåº¦å»¶è¿Ÿåˆ†æ:"
    kubectl get events --all-namespaces -o json | jq -r '
    .items[] |
    select(.reason == "Scheduled") |
    select(.firstTimestamp > (now - 3600)) |
    {
        pod: .involvedObject.name,
        namespace: .involvedObject.namespace,
        scheduled_time: .firstTimestamp,
        message: .message
    } |
    "\(.namespace)/\(.pod): \(.scheduled_time)"
    ' | head -20
    echo ""
    
    # 2. è°ƒåº¦å™¨æŒ‡æ ‡
    echo "2. è°ƒåº¦å™¨æŒ‡æ ‡ (å¦‚æœå¯ç”¨):"
    if kubectl get --raw /metrics 2>/dev/null | grep -q scheduler; then
        kubectl get --raw /metrics | grep -E "scheduler_(scheduling_duration|pending_pods|queue)"
    else
        echo "è°ƒåº¦å™¨æŒ‡æ ‡ä¸å¯ç”¨"
    fi
    echo ""
    
    # 3. èŠ‚ç‚¹èµ„æºç¢ç‰‡åŒ–åˆ†æ
    echo "3. èŠ‚ç‚¹èµ„æºç¢ç‰‡åŒ–:"
    kubectl describe nodes | grep -E "(Name:|Allocatable:|Allocated resources)" | \
        awk '/Name:/ {node=$2} /Allocatable:/ {print "èŠ‚ç‚¹:", node} /cpu/ {print "  CPU:", $0} /memory/ {print "  å†…å­˜:", $0}'
    echo ""
}

# ä¸»å‡½æ•°
main() {
    case "$1" in
        "pod")
            if [ -z "$2" ]; then
                echo "ç”¨æ³•: $0 pod <pod-name> [namespace]"
                exit 1
            fi
            debug_pod_scheduling $2 $3
            ;;
        "health")
            cluster_scheduling_health
            ;;
        "performance")
            scheduling_performance_analysis
            ;;
        "all")
            cluster_scheduling_health
            echo ""
            scheduling_performance_analysis
            ;;
        *)
            echo "ç”¨æ³•: $0 {pod|health|performance|all} [args...]"
            echo "ç¤ºä¾‹:"
            echo "  $0 pod my-pod default    # è¯Šæ–­ç‰¹å®š Pod"
            echo "  $0 health               # é›†ç¾¤è°ƒåº¦å¥åº·æ£€æŸ¥"
            echo "  $0 performance          # è°ƒåº¦æ€§èƒ½åˆ†æ"
            echo "  $0 all                  # å®Œæ•´è¯Šæ–­"
            exit 1
            ;;
    esac
}

main "$@"
```

#### 4.2.3 è°ƒåº¦å™¨æ—¥å¿—åˆ†æ

**æ—¥å¿—åˆ†æè„šæœ¬ï¼š**

```bash
#!/bin/bash
# scheduler-log-analyzer.sh

# åˆ†æè°ƒåº¦å™¨æ—¥å¿—ä¸­çš„é”™è¯¯æ¨¡å¼
analyze_scheduler_errors() {
    local log_file=${1:-"/var/log/scheduler/scheduler.log"}
    
    echo "=== è°ƒåº¦å™¨é”™è¯¯åˆ†æ ==="
    
    # 1. é”™è¯¯ç»Ÿè®¡
    echo "1. é”™è¯¯ç±»å‹ç»Ÿè®¡:"
    grep -E "(ERROR|WARN|Failed)" $log_file | \
        awk '{print $4}' | sort | uniq -c | sort -nr
    echo ""
    
    # 2. èµ„æºä¸è¶³é”™è¯¯
    echo "2. èµ„æºä¸è¶³é”™è¯¯:"
    grep "Insufficient" $log_file | tail -10
    echo ""
    
    # 3. äº²å’Œæ€§é”™è¯¯
    echo "3. äº²å’Œæ€§çº¦æŸé”™è¯¯:"
    grep -E "(affinity|anti-affinity)" $log_file | tail -10
    echo ""
    
    # 4. æ±¡ç‚¹å®¹å¿é”™è¯¯
    echo "4. æ±¡ç‚¹å®¹å¿é”™è¯¯:"
    grep "tolerate" $log_file | tail -10
    echo ""
    
    # 5. è°ƒåº¦å»¶è¿Ÿè­¦å‘Š
    echo "5. è°ƒåº¦å»¶è¿Ÿè­¦å‘Š:"
    grep "scheduling.*took" $log_file | tail -10
    echo ""
}

# å®æ—¶ç›‘æ§è°ƒåº¦å™¨æ—¥å¿—
monitor_scheduler_logs() {
    local scheduler_pod=$(kubectl get pods -n kube-system -l component=kube-scheduler -o jsonpath='{.items[0].metadata.name}')
    
    echo "ç›‘æ§è°ƒåº¦å™¨æ—¥å¿—: $scheduler_pod"
    echo "æŒ‰ Ctrl+C åœæ­¢ç›‘æ§"
    echo ""
    
    kubectl logs -n kube-system $scheduler_pod -f | \
        grep --line-buffered -E "(ERROR|WARN|Failed|Insufficient|affinity|tolerate)"
}

# ä¸»å‡½æ•°
main() {
    case "$1" in
        "analyze")
            analyze_scheduler_errors $2
            ;;
        "monitor")
            monitor_scheduler_logs
            ;;
        *)
            echo "ç”¨æ³•: $0 {analyze|monitor} [log-file]"
            echo "ç¤ºä¾‹:"
            echo "  $0 analyze /var/log/scheduler.log"
            echo "  $0 monitor"
            exit 1
            ;;
    esac
}

main "$@"
```

---

## 5. æ€»ç»“

### 5.1 è°ƒåº¦å™¨æ ¸å¿ƒæ¦‚å¿µå›é¡¾

Kubernetes è°ƒåº¦å™¨æ˜¯ä¸€ä¸ªå¤æ‚çš„**çº¦æŸä¼˜åŒ–ç³»ç»Ÿ**ï¼Œå…¶æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬ï¼š

- **ä¸¤é˜¶æ®µè°ƒåº¦**ï¼šè¿‡æ»¤ï¼ˆFilteringï¼‰+ è¯„åˆ†ï¼ˆScoringï¼‰
- **æ’ä»¶åŒ–æ¶æ„**ï¼š10 ä¸ªæ‰©å±•ç‚¹è¦†ç›–å®Œæ•´è°ƒåº¦ç”Ÿå‘½å‘¨æœŸ
- **å¤šç›®æ ‡ä¼˜åŒ–**ï¼šå¹³è¡¡èµ„æºåˆ©ç”¨ç‡ã€è´Ÿè½½å‡è¡¡ã€æœåŠ¡è´¨é‡ç­‰ç›®æ ‡
- **çº¦æŸæ»¡è¶³**ï¼šå¤„ç†ç¡¬çº¦æŸï¼ˆè¿‡æ»¤ï¼‰å’Œè½¯çº¦æŸï¼ˆè¯„åˆ†ï¼‰

### 5.2 è°ƒåº¦ç†è®ºè¦ç‚¹

1. **æ•°å­¦å»ºæ¨¡**ï¼šè°ƒåº¦é—®é¢˜æœ¬è´¨æ˜¯çº¦æŸä¼˜åŒ–é—®é¢˜
2. **ç®—æ³•å¤æ‚åº¦**ï¼šæ—¶é—´å¤æ‚åº¦ O(NÃ—F + MÃ—S)ï¼Œéœ€è¦æ€§èƒ½ä¼˜åŒ–
3. **å¤šç›®æ ‡æƒè¡¡**ï¼šé€šè¿‡å¸•ç´¯æ‰˜æœ€ä¼˜å®ç°ç›®æ ‡å¹³è¡¡
4. **å…¬å¹³æ€§ä¿è¯**ï¼šé€šè¿‡æ‹“æ‰‘åˆ†æ•£å’Œåäº²å’Œæ€§ç¡®ä¿è´Ÿè½½å‡è¡¡

### 5.3 å®è·µæŒ‡å¯¼åŸåˆ™

- **ç†è§£çº¦æŸå±‚æ¬¡**ï¼šç¡¬çº¦æŸï¼ˆå¿…é¡»æ»¡è¶³ï¼‰vs è½¯çº¦æŸï¼ˆä¼˜åŒ–ç›®æ ‡ï¼‰
- **åˆç†é…ç½®æƒé‡**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´è¯„åˆ†æ’ä»¶æƒé‡
- **ç›‘æ§è°ƒåº¦æ€§èƒ½**ï¼šå…³æ³¨è°ƒåº¦å»¶è¿Ÿã€æˆåŠŸç‡ã€èµ„æºåˆ©ç”¨ç‡
- **æ¸è¿›å¼ä¼˜åŒ–**ï¼šä»é»˜è®¤é…ç½®å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–è°ƒåº¦ç­–ç•¥
- **å®¡è®¡ç›‘æ§**ï¼šå®æ–½è°ƒåº¦å†³ç­–çš„å…¨é¢å®¡è®¡å’Œç›‘æ§æœºåˆ¶
- **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ pprof ç­‰å·¥å…·æ·±åº¦åˆ†æè°ƒåº¦å™¨æ€§èƒ½ç“¶é¢ˆ
- **æ•…éšœè¯Šæ–­**ï¼šå»ºç«‹ç³»ç»ŸåŒ–çš„è°ƒåº¦é—®é¢˜æ’æŸ¥å’Œåˆ†ææµç¨‹

é€šè¿‡æ·±å…¥ç†è§£è¿™äº›æ¦‚å¿µå’ŒåŸç†ï¼Œå¯ä»¥æ›´å¥½åœ°è®¾è®¡å’Œä¼˜åŒ– Kubernetes é›†ç¾¤çš„è°ƒåº¦ç­–ç•¥ï¼Œæé«˜åº”ç”¨æ€§èƒ½å’Œèµ„æºåˆ©ç”¨æ•ˆç‡ã€‚

---

## 6. æœ¯è¯­è¡¨

### A

**Affinityï¼ˆäº²å’Œæ€§ï¼‰ï¼š**

- Pod å¯¹èŠ‚ç‚¹æˆ–å…¶ä»– Pod çš„åå¥½æ€§è°ƒåº¦è§„åˆ™
- åŒ…æ‹¬èŠ‚ç‚¹äº²å’Œæ€§ï¼ˆNodeAffinityï¼‰å’Œ Pod äº²å’Œæ€§ï¼ˆPodAffinityï¼‰

**Anti-Affinityï¼ˆåäº²å’Œæ€§ï¼‰ï¼š**

- Pod é¿å…ä¸ç‰¹å®šèŠ‚ç‚¹æˆ–å…¶ä»– Pod è°ƒåº¦åœ¨ä¸€èµ·çš„è§„åˆ™
- ç”¨äºå®ç°å·¥ä½œè´Ÿè½½çš„åˆ†æ•£éƒ¨ç½²

### B

**Bindingï¼ˆç»‘å®šï¼‰ï¼š**

- å°† Pod åˆ†é…ç»™ç‰¹å®šèŠ‚ç‚¹çš„æœ€ç»ˆæ“ä½œ
- é€šè¿‡åˆ›å»º Binding å¯¹è±¡å®ç°

### C

**CycleStateï¼ˆè°ƒåº¦å‘¨æœŸçŠ¶æ€ï¼‰ï¼š**

- åœ¨å•æ¬¡è°ƒåº¦å‘¨æœŸä¸­ä¼ é€’æ•°æ®çš„çŠ¶æ€å¯¹è±¡
- ç”¨äºæ’ä»¶é—´çš„æ•°æ®å…±äº«

### E

**Extension Pointï¼ˆæ‰©å±•ç‚¹ï¼‰ï¼š**

- è°ƒåº¦æ¡†æ¶ä¸­å…è®¸æ’ä»¶ä»‹å…¥çš„ç‰¹å®šé˜¶æ®µ
- åŒ…æ‹¬ PreFilterã€Filterã€PostFilterã€PreScoreã€Scoreã€Reserveã€Permitã€PreBindã€Bindã€PostBind

### F

**Filterï¼ˆè¿‡æ»¤ï¼‰ï¼š**

- è°ƒåº¦è¿‡ç¨‹ä¸­ç­›é€‰å¯è°ƒåº¦èŠ‚ç‚¹çš„é˜¶æ®µ
- æ’é™¤ä¸æ»¡è¶³ Pod è¦æ±‚çš„èŠ‚ç‚¹

**Frameworkï¼ˆè°ƒåº¦æ¡†æ¶ï¼‰ï¼š**

- Kubernetes è°ƒåº¦å™¨çš„æ’ä»¶åŒ–æ¶æ„
- æä¾›æ‰©å±•ç‚¹å’Œæ’ä»¶ç®¡ç†æœºåˆ¶

### L

**Leader Electionï¼ˆé¢†å¯¼è€…é€‰ä¸¾ï¼‰ï¼š**

- å¤šè°ƒåº¦å™¨å®ä¾‹ä¸­é€‰æ‹©æ´»è·ƒè°ƒåº¦å™¨çš„æœºåˆ¶
- ç¡®ä¿é«˜å¯ç”¨æ€§å’Œé¿å…å†²çª

### N

**NodeAffinityï¼ˆèŠ‚ç‚¹äº²å’Œæ€§ï¼‰ï¼š**

- Pod å¯¹ç‰¹å®šèŠ‚ç‚¹ç‰¹å¾çš„åå¥½è§„åˆ™
- åŒ…æ‹¬ç¡¬æ€§è¦æ±‚ï¼ˆrequiredDuringSchedulingIgnoredDuringExecutionï¼‰å’Œè½¯æ€§åå¥½ï¼ˆpreferredDuringSchedulingIgnoredDuringExecutionï¼‰

**NodeSelectorï¼ˆèŠ‚ç‚¹é€‰æ‹©å™¨ï¼‰ï¼š**

- åŸºäºèŠ‚ç‚¹æ ‡ç­¾çš„ç®€å•è°ƒåº¦çº¦æŸ
- ç¡¬æ€§è¦æ±‚ï¼ŒPod åªèƒ½è°ƒåº¦åˆ°åŒ¹é…æ ‡ç­¾çš„èŠ‚ç‚¹

### P

**Pluginï¼ˆæ’ä»¶ï¼‰ï¼š**

- å®ç°ç‰¹å®šè°ƒåº¦é€»è¾‘çš„å¯æ’æ‹”ç»„ä»¶
- åœ¨è°ƒåº¦æ¡†æ¶çš„æ‰©å±•ç‚¹ä¸­æ‰§è¡Œ

**PodAffinityï¼ˆPod äº²å’Œæ€§ï¼‰ï¼š**

- Pod å€¾å‘äºä¸ç‰¹å®š Pod è°ƒåº¦åœ¨ç›¸åŒä½ç½®çš„è§„åˆ™
- åŸºäºæ‹“æ‰‘åŸŸï¼ˆå¦‚èŠ‚ç‚¹ã€å¯ç”¨åŒºï¼‰å®šä¹‰

**PodAntiAffinityï¼ˆPod åäº²å’Œæ€§ï¼‰ï¼š**

- Pod é¿å…ä¸ç‰¹å®š Pod è°ƒåº¦åœ¨ç›¸åŒä½ç½®çš„è§„åˆ™
- ç”¨äºå®ç°å·¥ä½œè´Ÿè½½çš„åˆ†æ•£éƒ¨ç½²

**Preemptionï¼ˆæŠ¢å ï¼‰ï¼š**

- é«˜ä¼˜å…ˆçº§ Pod é©±é€ä½ä¼˜å…ˆçº§ Pod ä»¥è·å¾—è°ƒåº¦æœºä¼šçš„æœºåˆ¶
- ç¡®ä¿å…³é”®å·¥ä½œè´Ÿè½½çš„è°ƒåº¦

**Priorityï¼ˆä¼˜å…ˆçº§ï¼‰ï¼š**

- Pod çš„è°ƒåº¦ä¼˜å…ˆçº§ï¼Œå½±å“è°ƒåº¦é¡ºåºå’ŒæŠ¢å è¡Œä¸º
- é€šè¿‡ PriorityClass å®šä¹‰

### Q

**QoSï¼ˆæœåŠ¡è´¨é‡ï¼‰ï¼š**

- åŸºäºèµ„æºè¯·æ±‚å’Œé™åˆ¶çš„ Pod åˆ†ç±»
- åŒ…æ‹¬ Guaranteedã€Burstableã€BestEffort ä¸‰ä¸ªç­‰çº§

**Queueï¼ˆè°ƒåº¦é˜Ÿåˆ—ï¼‰ï¼š**

- ç­‰å¾…è°ƒåº¦çš„ Pod é˜Ÿåˆ—
- åŒ…æ‹¬æ´»è·ƒé˜Ÿåˆ—ã€å¾…å¤„ç†é˜Ÿåˆ—å’Œä¸å¯è°ƒåº¦é˜Ÿåˆ—

### R

**Resource Quotaï¼ˆèµ„æºé…é¢ï¼‰ï¼š**

- å‘½åç©ºé—´çº§åˆ«çš„èµ„æºä½¿ç”¨é™åˆ¶
- å½±å“ Pod çš„è°ƒåº¦å’Œèµ„æºåˆ†é…

### S

**Schedulerï¼ˆè°ƒåº¦å™¨ï¼‰ï¼š**

- è´Ÿè´£å°† Pod åˆ†é…ç»™èŠ‚ç‚¹çš„ Kubernetes ç»„ä»¶
- é»˜è®¤è°ƒåº¦å™¨ä¸º kube-scheduler

**SchedulerNameï¼ˆè°ƒåº¦å™¨åç§°ï¼‰ï¼š**

- Pod è§„èŒƒä¸­æŒ‡å®šä½¿ç”¨çš„è°ƒåº¦å™¨
- æ”¯æŒå¤šè°ƒåº¦å™¨åœºæ™¯

**Scoreï¼ˆè¯„åˆ†ï¼‰ï¼š**

- è°ƒåº¦è¿‡ç¨‹ä¸­å¯¹å€™é€‰èŠ‚ç‚¹çš„æ‰“åˆ†é˜¶æ®µ
- é€‰æ‹©å¾—åˆ†æœ€é«˜çš„èŠ‚ç‚¹è¿›è¡Œè°ƒåº¦

### T

**Taintï¼ˆæ±¡ç‚¹ï¼‰ï¼š**

- èŠ‚ç‚¹ä¸Šçš„æ ‡è®°ï¼Œé˜»æ­¢ä¸å®¹å¿çš„ Pod è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹
- åŒ…æ‹¬ NoScheduleã€PreferNoScheduleã€NoExecute ä¸‰ç§æ•ˆæœ

**Tolerationï¼ˆå®¹å¿ï¼‰ï¼š**

- Pod å¯¹èŠ‚ç‚¹æ±¡ç‚¹çš„å®¹å¿é…ç½®
- å…è®¸ Pod è°ƒåº¦åˆ°æœ‰åŒ¹é…æ±¡ç‚¹çš„èŠ‚ç‚¹

**Topology Spread Constraintsï¼ˆæ‹“æ‰‘åˆ†å¸ƒçº¦æŸï¼‰ï¼š**

- æ§åˆ¶ Pod åœ¨æ‹“æ‰‘åŸŸé—´åˆ†å¸ƒçš„çº¦æŸè§„åˆ™
- å®ç°å·¥ä½œè´Ÿè½½çš„å‡åŒ€åˆ†å¸ƒ

### W

**Workloadï¼ˆå·¥ä½œè´Ÿè½½ï¼‰ï¼š**

- åœ¨ Kubernetes ä¸­è¿è¡Œçš„åº”ç”¨ç¨‹åº
- åŒ…æ‹¬ Deploymentã€StatefulSetã€DaemonSetã€Job ç­‰

---
